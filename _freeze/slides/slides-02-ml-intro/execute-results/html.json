{
  "hash": "0c39cabbd3ac19a16c58db1cdfac5826",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Introduction to Machine Learning\"\nformat:\n    revealjs:\n        html-math-method: mathjax\n        slide-number: true\n        slide-level: 2\n        theme:\n          - slides.scss\n        center: true\n        resources:\n          - data/\n          - img/\n---\n\n\n\n## Which cat is AI-generated?\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n![](img/ai-or-not-cat.png)\n\n[Source](https://thinktan.net/are-you-sure-this-photo-is-not-ai-generated/)\n:::\n\n::: {.column width=\"40%\"}\n\n- Which one do you think is AI-generated?\n    - A\n    - B\n    - Both\n    - None\n- What clues did you use to decide?    \n\n:::\n:::: \n\n## AI vs. ML vs. DL \n\n- What is AI, and how does it relate to Machine Learning (ML) and Deep Learning (DL)?\n\n![](img/ai-ml-dl.png)\n\n\n## Example: Image classification\n\n- Have you used search in Google Photos? You can search for \"cat\" and it will retrieve photos from your libraries containing cats.\n- This can be done using **image classification**. \n\n## Image classification\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n![](img/cat-or-fox.png) \n:::\n\n::: {.column width=\"40%\"}\n\n- Imagine you want to teach a robot to tell cats and foxes apart.\n- How would you approach it? \n::: \n\n::::\n\n## AI approach: example\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n![](img/cat-or-fox.png) \n:::\n\n::: {.column width=\"60%\"}\n- You hard-code rules: \"If the image has fur, whiskers, and pointy ears, it's a cat.\"\n- This works for normal cases, but what if the cat is missing an ear? Or if the fox has short fur?\n::: \n\n::::\n\n## ML approach: example {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n![](img/cat-or-fox.png) \n:::\n\n::: {.column width=\"60%\"}\n- We don’t tell the model the exact rule. Instead, we give it labeled examples, and it learns which features matter most.\n    - small nose ✅\n    - round face ✅\n    - whiskers ✅    \n- Instead of giving rules, we let the model figure out the best combination of features from data.    \n::: \n\n::::\n\n## DL approach: example {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n![](img/cat-or-fox.png) \n:::\n\n::: {.column width=\"60%\"}\n- The robot figures out the best features by itself using a neural network.\n- Instead of humans selecting features, the neural network extracts them automatically—from edges to textures to full shapes\n- The more data it sees, the better it gets—without human intervention.\n::: \n\n::::\n\n\n##  What is ML? \n\n\n- ML uses data to build models that identify patterns, make predictions, or generate content.\n- It enables computers to learn from data.\n- No single model is suitable for all situations.\n\n## When is ML suitable?\n\n- ML excels when the problem involve identifying complex patterns or relationships in large datasets that are difficult for humans to discern manually.\n- Rule-based systems are suitable where clear and deterministic rules can be defined. Good for structured decision making. \n- Human experts are good with problems which require deep contextual understanding, ethical judgment, creative input, or emotional intelligence.\n\n## Supervised learning\n\n- We wish to find a model function $f$ that relates $X$ to $y$.\n- We use the model function to predict targets of new examples. \n\n![](img/sup-learning.png){.nostretch fig-align=\"center\" width=\"700px\"}\n\n\n## Scenario\n\nImagine you're taking a course with four homework assignments and two quizzes. You’re feeling nervous about Quiz 2, so you want to predict your Quiz 2 grade based on your past performance. You collect data your friends who took the course in the past. \n\n## Terminology {.smaller}\n\nHere are a few rows from the data. \n\n![](img/sup-ML-terminology.png)\n\n- **Features:** relevant characteristics of the problem, usually suggested by experts  (typically denoted by $X$). \n- **Target:** the variable we want to predict (typically denoted by $y$). \n- **Example:** A row of feature values\n\n## Running example {.smaller}\n\n::: {#3f0c55ee .cell execution_count=2}\n``` {.python .cell-code}\ntoy_df = pd.read_csv(DATA_DIR + 'quiz2-grade-toy-regression.csv')\ntoy_df\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ml_experience</th>\n      <th>class_attendance</th>\n      <th>lab1</th>\n      <th>lab2</th>\n      <th>lab3</th>\n      <th>lab4</th>\n      <th>quiz1</th>\n      <th>quiz2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>92</td>\n      <td>93</td>\n      <td>84</td>\n      <td>91</td>\n      <td>92</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>94</td>\n      <td>90</td>\n      <td>80</td>\n      <td>83</td>\n      <td>91</td>\n      <td>84</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>78</td>\n      <td>85</td>\n      <td>83</td>\n      <td>80</td>\n      <td>80</td>\n      <td>82</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>91</td>\n      <td>94</td>\n      <td>92</td>\n      <td>91</td>\n      <td>89</td>\n      <td>92</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1</td>\n      <td>77</td>\n      <td>83</td>\n      <td>90</td>\n      <td>92</td>\n      <td>85</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>0</td>\n      <td>70</td>\n      <td>73</td>\n      <td>68</td>\n      <td>74</td>\n      <td>71</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>0</td>\n      <td>80</td>\n      <td>88</td>\n      <td>89</td>\n      <td>88</td>\n      <td>91</td>\n      <td>91</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n- Can you think of other relevant **features** for this problem? \n\n## Classification vs. Regression {.smaller}\n\n![](img/classification-vs-regression.png)\n\n## Training {.smaller}\n\n- In supervised ML, the goal is to learn a function that maps input features ($X$) to a target ($y$).\n- The relationship between $X$ and $y$ is often complex, making it difficult to define mathematically.\n- We use algorithms to approximate this complex relationship between $X$ and $y$.\n- **Training** is the process of applying an algorithm to learn the best function (or model) that maps $X$ to $y$. \n\n## Linear models {.smaller}\n\n:::: {.columns}\n\n:::{.column width=\"45%\"}\n- Linear models make an assumption that the relationship between `X` and `y` is linear. \n- In this case, with only one feature, our model is a straight line.\n- What do we need to represent a line?\n  - Slope ($w_1$): Determines the angle of the line.\n  - Y-intercept ($w_0$): Where the line crosses the y-axis.\n\n:::\n\n::: {.column width=\"55%\"}\n\n::: {#6d4e0b69 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](slides-02-ml-intro_files/figure-revealjs/cell-4-output-1.png){width=677 height=399}\n:::\n:::\n\n\n- Making predictions\n    - $y_{hat} = w_1 \\times \\text{\\# hours studied} + w_0$\n\n:::\n::::\n\n\n## Logistic regression {.smaller}\n- Suppose your target is binary: pass or fail \n- Logistic regression is used for such binary classification tasks.  \n- Logistic regression predicts a probability that the given example belongs to a particular class.\n- It uses **Sigmoid function** to map any real-valued input into a value between 0 and 1, representing the probability of a specific outcome.\n- A threshold (usually 0.5) is applied to the predicted probability to decide the final class label.  \n\n## Logistic regression {.smaller}\n\n:::: {.columns}\n\n:::{.column width=\"60%\"}\n\n::: {#b42bce43 .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](slides-02-ml-intro_files/figure-revealjs/cell-5-output-1.png){width=684 height=473}\n:::\n:::\n\n\n:::\n:::{.column width=\"40%\"}\n- Sigmoid Function: $\\hat{y} = \\sigma(w^\\top x_i + b) = \\frac{1}{1 + e^{-(w^\\top x_i + b)}}$\n- If you study $\\leq 3$ hours, you fail. \n- If you study $> 3$ hours, you pass. \n:::\n\n::::\n\n## A graphical view of a linear model {.smaller}\n:::: {.columns}\n\n:::{.column width=\"50%\"}\n\n::: {#93d84d10 .cell execution_count=5}\n\n::: {.cell-output .cell-output-display execution_count=5}\n![](slides-02-ml-intro_files/figure-revealjs/cell-6-output-1.svg){}\n:::\n:::\n\n\n:::\n\n:::{.column width=\"50%\"}\n\n- We have 4 features: x[0], x[1], x[2], x[3]\n- The output is calculated as $y = x[0]w[0] + x[1]w[1] + x[2]w[2] + x[3]w[3]$\n- For simplicity, we are ignoring the bias term. \n:::\n::::\n\n",
    "supporting": [
      "slides-02-ml-intro_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}