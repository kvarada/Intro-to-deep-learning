{
  "hash": "fea20cb9a0f9a17796bfd0aa2a785ab2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Introduction to Machine learning\"\nformat: \n    revealjs:\n      smaller: true\n      center: true\n---\n\n\n# What is Machine Learning (ML)?\n\n## Example: Image classification\n\n- Have you used search in Google Photos? You can search for \"my photos of cat\" and it will retrieve photos from your libraries containing cats.\n- This can be done using **image classification**, which is treated as a supervised learning problem. \n\n## Image classification\n\n- Imagine writing a Python program to differenciate between cats and foxes.  \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n![](img/cat-or-fox.png) \n:::\n\n::: {.column width=\"40%\"}\n- How would you approach it? \n::: \n\n::::\n\n## Traditional programming vs. ML {.smaller}\n\n- Traditional programming \n    - Ideal for problems where solutions can be derived through the direct application of established algorithms in a clear and predictable manner.\n    - Example: finding the shortest distance between two nodes (e.g., Dijkstra’s algorithm or the A* algorithm)\n- Machine learning\n    - Appropriate for scenarios where defining explicit rules or algorithms is challenging due to the complexity or scale of the data.\n    - Example: Distinguishing between images of cats and foxes by training a machine learning model to recognize patterns that differentiate the two.\n\n##  What is ML? \n\n- ML uses data to build models that identify patterns, make predictions, or generate content.\n- It enables computers to learn from data.\n- No single model is suitable for all situations.\n\n## Supervised learning\n\n- We wish to find a model function $f$ that relates $X$ to $y$.\n- We use the model function to predict targets of new examples. \n\n![](img/sup-learning.png){.nostretch fig-align=\"center\" width=\"700px\"}\n\n\n# ML vs. rule-based systems vs. human experts \n\n## iClicker 1.1: ML or not {.smaller}\n\nhttps://join.iclicker.com/YWOJ \n\n**Select all of the following statements which are suitable problems for machine learning.**\n\n- (A) Identifying objects within digital images, such as facial recognition in security systems or categorizing images based on content.\n- (B) Providing solutions for common technical issues in software or hardware, where a series of diagnostic steps can be predefined.\n- (C) Determining if individuals meet the necessary criteria for government or financial services based on strict guidelines.\n- (D) Identifying unusual patterns that may indicate fraudulent transactions in banking and finance.\n- (E) Automatically analyzing images from MRIs, CT scans, or X-rays to detect abnormalities like tumors or fractures.\n\n## iClicker 1.2: ML or not {.smaller}\n\nhttps://join.iclicker.com/YWOJ\n\n**Select all of the following statements which are suitable problems for human experts.**\n\n - (A) Ensuring that company practices adhere to established regulations and standards, where rules are clearly set out by legal or regulatory bodies.\n - (B) Predicting user preferences based on past behavior, crucial for services like Netflix or Amazon for suggesting movies or products.\n - (C) Evaluating legal cases where the context and subtleties of human behavior and law interpretation are crucial.\n - (D) Making decisions about end-of-life care or consent for surgery where ethical considerations are complex and deeply personal.\n - (E) Addressing mental health issues where human empathy, understanding, and adaptability are key.\n\n## Summary: When is ML suitable?\n\n- ML excels when the problem involve identifying complex patterns or relationships in large datasets that are difficult for humans to discern manually.\n- Rule-based systems are suitable where clear and deterministic rules can be defined. Good for structured decision making. \n- Human experts are good with problems which require deep contextual understanding, ethical judgment, creative input, or emotional intelligence.\n\n\n## Introduction to Machine Learning\n\\\nMachine Learning uses computer programs to digest and accurately model data. After *training* on the data, a program can be used to extract hidden patterns, make predictions in new situations or generate novel content.\n\nThe program learns based on the *features* present in the data, which represent the information we have about each example.\n\n## Introduction to Machine Learning\n\\ \n\n![](img/sup-ML-terminology.png)\n\n\n## Classification vs. Regression\n\\\n\n![](img/classification-vs-regression.png)\n\n## Measuring Performance\n\\\n\n* Performance on classification tasks can be measured based on the *accuracy* of the model's predictions.\n\n* Performance on a regression task can be measured based on *error*. Mean squared error is one choice, but there are many others!\n\n\n## Inference vs. Prediction\n\\\n\n* *Inference* is the use of a model to infer a relationship between features (independent variables) and targets (independent variables).\n\n* *Prediction* is the use of a model to predict the target value for a new example not seen in training.\n\n\n## Example: Linear Regression\n\\\n![](img/visualization.png)\n\n## Linear Classifiers\n\\\n\nWe can also build linear models for classification tasks. The idea is to convert the output from an arbitrary number to a number between 0 and 1, and treat it like a \"probability\".\n\nIn *logistic regression*, we squash the output using the sigmoid function and then adjust parameters (in training) to find the choice that makes the data \"most likely\".\n\n## Linear Classifiers\n\n![](img/us-map.png)\n\nCan you guess what this dataset is?\n\n## Linear Classifiers\n\n![](img/logistic.png)\n\nLogistic Regression predicts a *linear* decision boundary.\n\n## Sentiment Analysis: An Example\n\n\n\n\\\nLet us attempt to use logistic regression to do sentiment analysis on a database of IMDB reviews. The database is available [here](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download).\n\n::: {#cb3d968f .cell execution_count=2}\n``` {.python .cell-code}\nimdb_df = pd.read_csv(\"data/imdb_master.csv\", encoding=\"ISO-8859-1\")\nimdb_df.rename(columns={\"sentiment\": \"label\"}, inplace = True)\n```\n:::\n\n\n::: {#a59d00fc .cell execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe will use only about 10% of the dataset for training (to speed things up)\n\n\n\n## Bag of Words\n\\\nTo create features that logistic regression can use, we will represent these reviews via a \"bag of words\" strategy.\n\nWe create a new feature for every word that appears in the dataset. Then, if a review contains that word exactly once, the corresponding feature gets a value of 1 for that review. If the word appears four times, the feature gets a value of 4. If the word is not present, it's marked as 0.\n\n## Bag of Words\n\\\nNotice that the result is a sparse matrix. Most reviews contain only a small number of words.\n\n::: {#20ce8d9f .cell execution_count=5}\n``` {.python .cell-code}\nvec = CountVectorizer(stop_words=\"english\")\nbow = vec.fit_transform(X_train)\nbow\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<Compressed Sparse Row sparse matrix of dtype 'int64'\n\twith 439384 stored elements and shape (5000, 38867)>\n```\n:::\n:::\n\n\nThere are a total of 38867 \"words\" among the reviews. Here are some of them: \n\n::: {#626e6a59 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\narray(['00', 'affection', 'apprehensive', 'barbara', 'blore',\n       'businessman', 'chatterjee', 'commanding', 'cramped', 'defining',\n       'displaced', 'edie', 'evolving', 'fingertips', 'gaffers',\n       'gravitas', 'heist', 'iliad', 'investment', 'kidnappee',\n       'licentious', 'malã', 'mice', 'museum', 'obsessiveness',\n       'parapsychologist', 'plasters', 'property', 'reclined',\n       'ridiculous', 'sayid', 'shivers', 'sohail', 'stomaches', 'syrupy',\n       'tolerance', 'unbidden', 'verneuil', 'wilcox'], dtype=object)\n```\n:::\n:::\n\n\n## Checking the class counts\n\nLet us see how many reviews are positive, and how many are negative.\n\\ \n\n::: {#f0b92e61 .cell execution_count=7}\n``` {.python .cell-code}\ny_train.value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nlabel\npositive    2517\nnegative    2483\nName: count, dtype: int64\n```\n:::\n:::\n\n\n\\\n\nThe dataset looks pretty balanced, so a classifier predicting at random would at best guess about 50% correctly.\n\nWe will not train our model.\n\n# Testing Performance\n\n## Testing Performance\n\\\n\nLet's see how the model performs after training.\n\n::: {#50890272 .cell execution_count=8}\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fit_time</th>\n      <th>score_time</th>\n      <th>test_score</th>\n      <th>train_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.404817</td>\n      <td>0.057180</td>\n      <td>0.828</td>\n      <td>0.99975</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.413377</td>\n      <td>0.060267</td>\n      <td>0.830</td>\n      <td>0.99975</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.423920</td>\n      <td>0.059358</td>\n      <td>0.848</td>\n      <td>0.99975</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.406662</td>\n      <td>0.057710</td>\n      <td>0.833</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.401656</td>\n      <td>0.058953</td>\n      <td>0.840</td>\n      <td>0.99975</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n\\\nWe're able to predict with roughly 84% accuracy on validation sets. Looks like our model learned something!\n\n## Tuning hyperparameters\n\\\n\nHowever, the training scores are perfect (and higher than validation scores) so our model is likely overfitting.\n\nMaybe it just memorized some rare words, each appearing only in one review, and associated these with the review's label. We could try reducing the size of our dictionary to prevent this.\n\n## Tuning hyperparameters\n\\\n\nThere are many tools available to automate the search for good hyperparameters. These can make our life easy, but there is always the danger of optimization bias in the results.\n\n\n\n## Investigating the model\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\\\n\nLet's see what associations our model learned.\n\n::: {#964db1aa .cell execution_count=10}\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Coefficient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>excellent</th>\n      <td>1.119168</td>\n    </tr>\n    <tr>\n      <th>surprised</th>\n      <td>1.030143</td>\n    </tr>\n    <tr>\n      <th>perfect</th>\n      <td>0.994759</td>\n    </tr>\n    <tr>\n      <th>wonderful</th>\n      <td>0.980567</td>\n    </tr>\n    <tr>\n      <th>amazing</th>\n      <td>0.933009</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>boring</th>\n      <td>-1.130079</td>\n    </tr>\n    <tr>\n      <th>terrible</th>\n      <td>-1.148760</td>\n    </tr>\n    <tr>\n      <th>waste</th>\n      <td>-1.152334</td>\n    </tr>\n    <tr>\n      <th>awful</th>\n      <td>-1.402264</td>\n    </tr>\n    <tr>\n      <th>worst</th>\n      <td>-1.738195</td>\n    </tr>\n  </tbody>\n</table>\n<p>33535 rows × 1 columns</p>\n</div>\n```\n:::\n:::\n\n\n:::\n\n## Investigating the model\n\\\n\nThey make sense! Let's visualize the 20 most important features.\n\n::: {#a9ffc673 .cell execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![](slides-02-ml-intro_files/figure-revealjs/cell-12-output-1.png){width=1202 height=489}\n:::\n:::\n\n\n## Making Predictions\n\\\n\nFinally, let's try predicting on some new examples.\n\\\n\n::: {#6bfc25b1 .cell execution_count=12}\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n['It got a bit boring at times but the direction was excellent and the acting was flawless. Overall I enjoyed the movie and I highly recommend it!',\n 'The plot was shallower than a kiddie pool in a drought, but hey, at least we now know emojis should stick to texting and avoid the big screen.']\n```\n:::\n:::\n\n\nHere are the model predictions:\n\n::: {#62fab417 .cell execution_count=13}\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\narray(['positive', 'negative'], dtype=object)\n```\n:::\n:::\n\n\n\\\n\nLet's see which vocabulary words were present in the first review, and how they contributed to the classification.\n\n## Understanding Predictions\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#322a6691 .cell execution_count=14}\n\n::: {.cell-output .cell-output-stdout}\n```\nIt got a bit boring at times but the direction was excellent and the acting was flawless. Overall I enjoyed the movie and I highly recommend it!\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-02-ml-intro_files/figure-revealjs/cell-15-output-2.png){width=1223 height=428}\n:::\n:::\n\n\n:::\n\n\n## Summary\n\\\n\nThe bag-of-words representation was very simple-- we only counted which words appeared in which reviews. There was no attempt to maintain syntactical or grammatical structure or to study correlations between words.\n\nWe also trained on just 5000 examples. Nevertheless, our model performs quite well.\n\n\n\n## Linear Models\n\\\n\nPros:\n\n* Easy to train and to interpret\n* Widely applicable despite some strong assumptions\n* If you have a regression task, check whether a linear regression is already good enough! If you have a classification task, logistic regression is a go-to first option.\n\nCons:\n\n* Strong assumptions\n* Linear decision boundaries for classifiers\n* Correlated features can cause problems\n\n",
    "supporting": [
      "slides-02-ml-intro_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}