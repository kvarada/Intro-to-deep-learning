---
title: "Introduction to Machine Learning"
format:
    revealjs:
        html-math-method: mathjax
        slide-number: true
        slide-level: 2
        theme:
          - slides.scss
        center: true
        resources:
          - data/
          - img/
---

```{python}
import os
import sys

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

sys.path.append(os.path.join(os.path.abspath("."), "code"))
from plotting_functions import *
from utils import *
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler
from sklearn.svm import SVC
from sklearn.datasets import make_blobs, make_classification
DATA_DIR = os.path.join(os.path.abspath("."), "data/")

```

## Which cat is AI-generated?

:::: {.columns}
::: {.column width="60%"}

![](img/ai-or-not-cat.png)

[Source](https://thinktan.net/are-you-sure-this-photo-is-not-ai-generated/)
:::

::: {.column width="40%"}

- Which one do you think is AI-generated?
    - A
    - B
    - Both
    - None
:::
:::: 



# What is Machine Learning (ML)?

## Example: Image classification

- Have you used search in Google Photos? You can search for "my photos of cat" and it will retrieve photos from your libraries containing cats.
- This can be done using **image classification**, which is treated as a supervised learning problem. 

## Image classification

:::: {.columns}

::: {.column width="60%"}
![](img/cat-or-fox.png) 
:::

::: {.column width="40%"}
- Imagine writing a Python program to differenciate between cats and foxes.  
- How would you approach it? 
::: 

::::

## Traditional programming vs. ML {.smaller}

- Traditional programming 
    - Ideal for problems where solutions can be derived through the direct application of established algorithms in a clear and predictable manner.
    - Example: finding the shortest distance between two nodes (e.g., Dijkstra’s algorithm or the A* algorithm)
- Machine learning
    - Appropriate for scenarios where defining explicit rules or algorithms is challenging due to the complexity or scale of the data.
    - Example: Distinguishing between images of cats and foxes by training a machine learning model to recognize patterns that differentiate the two.

##  What is ML? 

- ML uses data to build models that identify patterns, make predictions, or generate content.
- It enables computers to learn from data.
- No single model is suitable for all situations.

## When is ML suitable?

- ML excels when the problem involve identifying complex patterns or relationships in large datasets that are difficult for humans to discern manually.
- Rule-based systems are suitable where clear and deterministic rules can be defined. Good for structured decision making. 
- Human experts are good with problems which require deep contextual understanding, ethical judgment, creative input, or emotional intelligence.

## Supervised learning

- We wish to find a model function $f$ that relates $X$ to $y$.
- We use the model function to predict targets of new examples. 

![](img/sup-learning.png){.nostretch fig-align="center" width="700px"}


## Scenario

Imagine you're taking a course with four homework assignments and two quizzes. You’re feeling nervous about Quiz 2, so you want to predict your Quiz 2 grade based on your past performance. You collect data your friends who took the course in the past. 

## Terminology {.smaller}

Here are a few rows from the data. 

![](img/sup-ML-terminology.png)

- **Features:** relevant characteristics of the problem, usually suggested by experts  (typically denoted by $X$). 
- **Target:** the variable we want to predict (typically denoted by $y$). 
- **Example:** A row of feature values

## Running example {.smaller}
```{python}
#| echo: true
toy_df = pd.read_csv(DATA_DIR + 'quiz2-grade-toy-regression.csv')
toy_df
```
- Can you think of other relevant **features** for this problem? 

## Classification vs. Regression {.smaller}

![](img/classification-vs-regression.png)

## Training {.smaller}

- In supervised ML, the goal is to learn a function that maps input features ($X$) to a target ($y$).
- The relationship between $X$ and $y$ is often complex, making it difficult to define mathematically.
- We use algorithms to approximate this complex relationship between $X$ and $y$.
- **Training** is the process of applying an algorithm to learn the best function (or model) that maps $X$ to $y$. 

## Linear models {.smaller}

:::: {.columns}

:::{.column width="45%"}
- Linear models make an assumption that the relationship between `X` and `y` is linear. 
- In this case, with only one feature, our model is a straight line.
- What do we need to represent a line?
  - Slope ($w_1$): Determines the angle of the line.
  - Y-intercept ($w_0$): Where the line crosses the y-axis.

:::

::: {.column width="55%"}
```{python}
import matplotlib.pyplot as plt
import numpy as np
# Data
hours_studied = [0.5, 1.0, 2.0, 2.0, 3.5, 4.0, 5.5, 6.0]
grades = [25, 35, 70, 40, 60, 55, 75, 80]

# Convert data to numpy arrays and reshape for model fitting
X = np.array(hours_studied).reshape(-1, 1)
y = np.array(grades)

from sklearn.linear_model import LinearRegression
# Fit a linear regression model
model = LinearRegression()
model.fit(X, y)

# Generate predictions for plotting the regression line
X_range = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
y_pred = model.predict(X_range)

# Plotting the data points and the regression line
plt.figure(figsize=(8, 4))
plt.scatter(X, y, color='green', edgecolors='black', s=130, label='Data Points')
plt.plot(X_range, y_pred, color='blue', linewidth=2, label='Regression Line')
plt.xlabel('# hours studied')
plt.ylabel('% grade in Quiz 2')
plt.title('Linear Models: Prediction')
# plt.legend()
plt.grid(True)
plt.show()
```
- Making predictions
    - $y_{hat} = w_1 \times \text{\# hours studied} + w_0$

:::
::::


## Logistic regression {.smaller}
- Suppose your target is binary: pass or fail 
- Logistic regression is used for such binary classification tasks.  
- Logistic regression predicts a probability that the given example belongs to a particular class.
- It uses **Sigmoid function** to map any real-valued input into a value between 0 and 1, representing the probability of a specific outcome.
- A threshold (usually 0.5) is applied to the predicted probability to decide the final class label.  

## Logistic regression {.smaller}

:::: {.columns}

:::{.column width="60%"}

```{python}
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LogisticRegression

# Data
hours_studied = [0.5, 1.0, 2.0, 2.0, 3.5, 4.0, 5.5, 6.0]
grades = ['fail', 'fail', 'pass', 'fail', 'pass', 'fail', 'pass', 'pass']

# Converting target variable to binary (0: fail, 1: pass)
grades_binary = [1 if grade == 'pass' else 0 for grade in grades]

# Reshape the data to fit the model
X = np.array(hours_studied).reshape(-1, 1)
y = np.array(grades_binary)

# Fit logistic regression model
model = LogisticRegression()
model.fit(X, y)

# Generate a range of hours studied for prediction
X_test = np.linspace(min(hours_studied), max(hours_studied), 100).reshape(-1, 1)

# Predict probabilities
probabilities = model.predict_proba(X_test)[:, 1]

# Find decision boundary (where probability = 0.5)
decision_boundary = X_test[np.isclose(probabilities, 0.5, atol=0.01)][0]

# Plotting
plt.figure(figsize=(8, 5))
plt.scatter(hours_studied, y, color='red', label='0=Fail, 1=Pass')
plt.plot(X_test, probabilities, label='Prediction Probability', color='blue')
plt.axvline(x=decision_boundary, ymin=0, ymax=0.5, color='green', linestyle='--')
plt.hlines(y=0.5, xmin=min(hours_studied), xmax=decision_boundary, color='green', linestyle='--')
# Decision boundary (dotted vertical line)
plt.axvline(x=decision_boundary, color='green', linestyle='-', linewidth=4, label='Decision Boundary (0.5)')
plt.xlabel('Hours Studied')
plt.ylabel('Probability of Passing')
plt.title('Logistic Regression Prediction Probabilities')
plt.legend(loc="best", fontsize="small")
plt.grid(True)
plt.show()
```
:::
:::{.column width="40%"}
- Sigmoid Function: $\hat{y} = \sigma(w^\top x_i + b) = \frac{1}{1 + e^{-(w^\top x_i + b)}}$
- If you study $\leq 3$ hours, you fail. 
- If you study $> 3$ hours, you pass. 
:::

::::

## A graphical representation of a linear model {.smaller}
:::: {.columns}

:::{.column width="50%"}
```{python}
import mglearn

mglearn.plots.plot_logistic_regression_graph()
```
:::

:::{.column width="50%"}

- We have 4 features: x[0], x[1], x[2], x[3]
- The output is calculated as $y = x[0]w[0] + x[1]w[1] + x[2]w[2] + x[3]w[3]$
- For simplicity, we are ignoring the bias term. 
:::
::::
