---
title: "Introduction to Machine learning"
format: 
    revealjs:
      smaller: true
      center: true
---

## Which cat is AI-generated?

:::: {.columns}
::: {.column width="60%"}

![](img/ai-or-not-cat.png)

[Source](https://thinktan.net/are-you-sure-this-photo-is-not-ai-generated/)
:::

::: {.column width="40%"}

- Which one do you think is AI-generated?
    - A
    - B
    - Both
    - None
:::
:::: 



# What is Machine Learning (ML)?

## Example: Image classification

- Have you used search in Google Photos? You can search for "my photos of cat" and it will retrieve photos from your libraries containing cats.
- This can be done using **image classification**, which is treated as a supervised learning problem. 

## Image classification

:::: {.columns}

::: {.column width="60%"}
![](img/cat-or-fox.png) 
:::

::: {.column width="40%"}
- Imagine writing a Python program to differenciate between cats and foxes.  
- How would you approach it? 
::: 

::::

## Traditional programming vs. ML {.smaller}

- Traditional programming 
    - Ideal for problems where solutions can be derived through the direct application of established algorithms in a clear and predictable manner.
    - Example: finding the shortest distance between two nodes (e.g., Dijkstraâ€™s algorithm or the A* algorithm)
- Machine learning
    - Appropriate for scenarios where defining explicit rules or algorithms is challenging due to the complexity or scale of the data.
    - Example: Distinguishing between images of cats and foxes by training a machine learning model to recognize patterns that differentiate the two.

##  What is ML? 

- ML uses data to build models that identify patterns, make predictions, or generate content.
- It enables computers to learn from data.
- No single model is suitable for all situations.

## Supervised learning

- We wish to find a model function $f$ that relates $X$ to $y$.
- We use the model function to predict targets of new examples. 

![](img/sup-learning.png){.nostretch fig-align="center" width="700px"}


# ML vs. rule-based systems vs. human experts 

## iClicker 1.1: ML or not {.smaller}

https://join.iclicker.com/YWOJ 

**Select all of the following statements which are suitable problems for machine learning.**

- (A) Identifying objects within digital images, such as facial recognition in security systems or categorizing images based on content.
- (B) Providing solutions for common technical issues in software or hardware, where a series of diagnostic steps can be predefined.
- (C) Determining if individuals meet the necessary criteria for government or financial services based on strict guidelines.
- (D) Identifying unusual patterns that may indicate fraudulent transactions in banking and finance.
- (E) Automatically analyzing images from MRIs, CT scans, or X-rays to detect abnormalities like tumors or fractures.

## iClicker 1.2: ML or not {.smaller}

https://join.iclicker.com/YWOJ

**Select all of the following statements which are suitable problems for human experts.**

 - (A) Ensuring that company practices adhere to established regulations and standards, where rules are clearly set out by legal or regulatory bodies.
 - (B) Predicting user preferences based on past behavior, crucial for services like Netflix or Amazon for suggesting movies or products.
 - (C) Evaluating legal cases where the context and subtleties of human behavior and law interpretation are crucial.
 - (D) Making decisions about end-of-life care or consent for surgery where ethical considerations are complex and deeply personal.
 - (E) Addressing mental health issues where human empathy, understanding, and adaptability are key.

## Summary: When is ML suitable?

- ML excels when the problem involve identifying complex patterns or relationships in large datasets that are difficult for humans to discern manually.
- Rule-based systems are suitable where clear and deterministic rules can be defined. Good for structured decision making. 
- Human experts are good with problems which require deep contextual understanding, ethical judgment, creative input, or emotional intelligence.


## Introduction to Machine Learning
\
Machine Learning uses computer programs to digest and accurately model data. After *training* on the data, a program can be used to extract hidden patterns, make predictions in new situations or generate novel content.

The program learns based on the *features* present in the data, which represent the information we have about each example.

## Introduction to Machine Learning
\ 

![](img/sup-ML-terminology.png)


## Classification vs. Regression
\

![](img/classification-vs-regression.png)

## Measuring Performance
\

* Performance on classification tasks can be measured based on the *accuracy* of the model's predictions.

* Performance on a regression task can be measured based on *error*. Mean squared error is one choice, but there are many others!


## Inference vs. Prediction
\

* *Inference* is the use of a model to infer a relationship between features (independent variables) and targets (independent variables).

* *Prediction* is the use of a model to predict the target value for a new example not seen in training.


## Example: Linear Regression
\
![](img/visualization.png)

## Linear Classifiers
\

We can also build linear models for classification tasks. The idea is to convert the output from an arbitrary number to a number between 0 and 1, and treat it like a "probability".

In *logistic regression*, we squash the output using the sigmoid function and then adjust parameters (in training) to find the choice that makes the data "most likely".

## Linear Classifiers

![](img/us-map.png)

Can you guess what this dataset is?

## Linear Classifiers

![](img/logistic.png)

Logistic Regression predicts a *linear* decision boundary.

## Sentiment Analysis: An Example

```{python}
import sys, os
sys.path.append(os.path.join(os.path.abspath("."), "code"))
from sup_learning import *
```
\
Let us attempt to use logistic regression to do sentiment analysis on a database of IMDB reviews. The database is available [here](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download).

```{python}
#| echo: True
imdb_df = pd.read_csv("data/imdb_master.csv", encoding="ISO-8859-1")
imdb_df.rename(columns={"sentiment": "label"}, inplace = True)
```
```{python}
imdb_df = imdb_df[imdb_df["label"].str.startswith(("pos", "neg"))]
imdb_df.head()
```

We will use only about 10% of the dataset for training (to speed things up)

```{python}
imdb_df["review_pp"] = imdb_df["review"].apply(replace_tags)
train_df, test_df = train_test_split(imdb_df, test_size=0.9, random_state=123)
X_train, y_train = train_df["review_pp"], train_df["label"]
X_test, y_test = test_df["review_pp"], test_df["label"]
```

## Bag of Words
\
To create features that logistic regression can use, we will represent these reviews via a "bag of words" strategy.

We create a new feature for every word that appears in the dataset. Then, if a review contains that word exactly once, the corresponding feature gets a value of 1 for that review. If the word appears four times, the feature gets a value of 4. If the word is not present, it's marked as 0.

## Bag of Words
\
Notice that the result is a sparse matrix. Most reviews contain only a small number of words.

```{python}
#| echo: True
vec = CountVectorizer(stop_words="english")
bow = vec.fit_transform(X_train)
bow
```

There are a total of 38867 "words" among the reviews. Here are some of them: 

```{python}
vocab = vec.get_feature_names_out()
vocab[::1000]
```

## Checking the class counts

Let us see how many reviews are positive, and how many are negative.
\ 

```{python}
#| echo: True
y_train.value_counts()
```
\

The dataset looks pretty balanced, so a classifier predicting at random would at best guess about 50% correctly.

We will not train our model.

# Testing Performance

## Testing Performance
\

Let's see how the model performs after training.

```{python}
pipe_lr = make_pipeline(
    CountVectorizer(stop_words="english"),
    LogisticRegression(max_iter=1000),
)
scores = cross_validate(pipe_lr, X_train, y_train, return_train_score=True)
pd.DataFrame(scores)
```
\
We're able to predict with roughly 84% accuracy on validation sets. Looks like our model learned something!

## Tuning hyperparameters
\

However, the training scores are perfect (and higher than validation scores) so our model is likely overfitting.

Maybe it just memorized some rare words, each appearing only in one review, and associated these with the review's label. We could try reducing the size of our dictionary to prevent this.

## Tuning hyperparameters
\

There are many tools available to automate the search for good hyperparameters. These can make our life easy, but there is always the danger of optimization bias in the results.


```{python}

from scipy.stats import loguniform, randint, uniform
from sklearn.model_selection import RandomizedSearchCV

param_dist = {
    "countvectorizer__max_features": randint(10, len(vocab)),
    "logisticregression__C": loguniform(1e-3, 1e3)
}
pipe_lr = make_pipeline(CountVectorizer(stop_words="english"), LogisticRegression(max_iter=1000))
random_search = RandomizedSearchCV(pipe_lr, param_dist, n_iter=10, n_jobs=-1, return_train_score=True)
random_search.fit(X_train, y_train)

best_model = random_search.best_estimator_

```

## Investigating the model
::: {.scroll-container style="overflow-y: scroll; height: 400px;"}
\

Let's see what associations our model learned.

```{python}
#| code-overflow: scroll

# Get feature names
feature_names = best_model.named_steps['countvectorizer'].get_feature_names_out().tolist()

# Get coefficients 
coeffs = best_model.named_steps["logisticregression"].coef_.flatten()

word_coeff_df = pd.DataFrame(coeffs, index=feature_names, columns=["Coefficient"])
word_coeff_df.sort_values(by="Coefficient", ascending=False)
```
:::

## Investigating the model
\

They make sense! Let's visualize the 20 most important features.

```{python}
mglearn.tools.visualize_coefficients(coeffs, feature_names, n_top_features=20)
```

## Making Predictions
\

Finally, let's try predicting on some new examples.
\

```{python}
fake_reviews = ["It got a bit boring at times but the direction was excellent and the acting was flawless. Overall I enjoyed the movie and I highly recommend it!",
 "The plot was shallower than a kiddie pool in a drought, but hey, at least we now know emojis should stick to texting and avoid the big screen."
]
fake_reviews
```

Here are the model predictions:

```{python}
best_model.predict(fake_reviews)
```

\

Let's see which vocabulary words were present in the first review, and how they contributed to the classification.

## Understanding Predictions
::: {.scroll-container style="overflow-y: scroll; height: 400px;"}
```{python}
plot_coeff_example(best_model, fake_reviews[0], coeffs, feature_names)
```
:::


## Summary
\

The bag-of-words representation was very simple-- we only counted which words appeared in which reviews. There was no attempt to maintain syntactical or grammatical structure or to study correlations between words.

We also trained on just 5000 examples. Nevertheless, our model performs quite well.



## Linear Models
\

Pros:

* Easy to train and to interpret
* Widely applicable despite some strong assumptions
* If you have a regression task, check whether a linear regression is already good enough! If you have a classification task, logistic regression is a go-to first option.

Cons:

* Strong assumptions
* Linear decision boundaries for classifiers
* Correlated features can cause problems