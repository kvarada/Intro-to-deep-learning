{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3172,"sourceType":"datasetVersion","datasetId":1833}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exercise 1 - Building a Decision Tree","metadata":{}},{"cell_type":"markdown","source":"First, we need to load in our dataset. We will be using the `spotifyclassification` (available [here](https://www.kaggle.com/datasets/geomack/spotifyclassification)) for this exercise.\n\nUse the \"Add Input\" button in the panel on the right to make these datasets available to your notebook. If you have successfully added these datasets, then running the cell below should print the path you can use to load them in.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The cell below imports some packages that we will use for building and evaluating our models.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import cross_val_score, cross_validate, train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reading in the data","metadata":{}},{"cell_type":"markdown","source":"Okay, we're ready to get started. Let's load in our first dataset and take a look at it!","metadata":{}},{"cell_type":"code","source":"spotify_df = pd.read_csv(\"/kaggle/input/data.csv\", index_col=0)\nspotify_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The dataset contains a number of features of songs from 2017 available on Spotify, and a binary variable `target` that represents whether the user liked the song (encoded as 1) or not (encoded as 0).\n\nThese are *one* user's listening preferences. We will investigate whether the features Spotify provides are helpful in predicting whether or not a user likes a particular song.","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">\n    \n**Discussion questions**\n1. Look up the documentation of the features [here](https://developer.spotify.com/documentation/web-api/reference/get-audio-features). Which ones do you think are likely to help with predicting our **target** column? Which ones are best discarded?\n\n2. Is this is supervised, unsupervised or self-supervised machine learning problem? Do we want to perform inference or prediction?\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\">\n\nType your answer below.\n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"## Data Splitting","metadata":{}},{"cell_type":"markdown","source":"As always, our first task should be to split our data into a training and test set. We will not access the test data until the very end, when we are ready to evaluate our final trained model.","metadata":{}},{"cell_type":"code","source":"# Split the data\n\ntrain_df, test_df = train_test_split(spotify_df, test_size=0.2, random_state=123)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can check how many data points our training and test sets contain","metadata":{}},{"cell_type":"code","source":"n_train_samples = train_df.shape[0]\nn_test_samples = test_df.shape[0]\n\nprint(f\"Training Examples: {n_train_samples}\")\nprint(f\"Test Examples: {n_test_samples}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Pandas also allows us to conveniently calculate some summary statistics for our dataset. For example, we can check what the mean and standard deviation for our numerical columns is.","metadata":{}},{"cell_type":"code","source":"# Summary statistics\n\nspotify_summary = train_df.describe()\nspotify_summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"We will first try to understand our dataset by examining some features directly. The code below produces histograms for the `loudness` feature using pandas plotting. The histograms show the distribution of the feature values in the training set, separated for positive (target=1, i.e., user liked the song) and negative (target=0, i.e., user disliked the song) examples.\n\nThere are two different histograms, one for target = 0 and one for target = 1, and they are overlaid on top of each other.","metadata":{}},{"cell_type":"code","source":"# Plotting the loudness feature\n\nfeat = \"loudness\"\ntrain_df.groupby(\"target\")[feat].plot.hist(bins=50, alpha=0.5, legend=True, density = True, title = \"Histogram of \" + feat);\nplt.xlabel(feat);\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The histogram shows that extremely quiet songs tend to be disliked (more blue bars than orange on the left) and very loud songs also tend to be disliked (more blue than orange on the far right).","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">\n    \n**Discussion questions**\n\nSuppose we were to build a decision tree using only the `loudness` feature. What would a good decision tree look like? What threshold values is our model likely to learn? How deep should we make our tree?\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\">\n\nType your answer below.\n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"## More plotting\n\nThe code below makes similar plots for some other features-- specifically `acousticness`, `danceability`, `tempo`, `energy` and `valence`. Run the cell and examine the histograms generated. Feel free to include more features in the list to be plotted.","metadata":{}},{"cell_type":"code","source":"features = [\n    \"acousticness\",\n    \"danceability\",\n    \"tempo\",\n    \"energy\",\n    \"valence\",\n]\nfor feat in features:\n    ax = train_df.groupby(\"target\")[feat].plot.hist(bins=50, alpha=0.5, legend=True, density = True, title=\"Histogram of \" + feat)\n    plt.xlabel(feat)\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">\n    \n**Discussion questions**\n\nSome features (like `tempo`) seem to have a similar distribution between both target classes (0 and 1). Should we drop these features from our data, based on the histograms we plotted?\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\">\n\nType your answer below.\n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"We are going to drop `song_title` and `artist` since they are text features. (**Do you expect either feature to be useful?**)\n\nWe are also separating the features from the target.","metadata":{}},{"cell_type":"code","source":"X_train = train_df.drop(columns=[\"target\", \"song_title\", \"artist\"])\ny_train = train_df[\"target\"]\n\nX_test = test_df.drop(columns=[\"target\", \"song_title\", \"artist\"])\ny_test = test_df[\"target\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dummy Classifier\n\nFirst, we train a dummy classifier. This gives us a baseline prediction against which to compare our model performance.\n\nThe dummy classifier predicts the same value of the target for all examples in the training set. When we call `fit()`, our dummy classifier learns the most common value of the target. The function `score()` then predicts this value for all examples, and returns the proportion which were correctly predicted. So the value returned here is the proportion of examples that belong to the most common target class.","metadata":{}},{"cell_type":"code","source":"# Dummy Classifier\n\ndummy = DummyClassifier(random_state=123)\ndummy.fit(X_train, y_train)\ndummy.score(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Looks like a balanced dataset! We have a close to 50-50 split between the target classes. Any ML model that actually learned useful information should perform with an accuracy higher than 50%.\n\nWe are now ready to build our decision tree.","metadata":{}},{"cell_type":"code","source":"# Creating a decision tree\n\nspotify_tree = DecisionTreeClassifier(random_state=123)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's train and score our tree.","metadata":{}},{"cell_type":"code","source":"spotify_tree.fit(X_train, y_train)\nspotify_tree.score(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"That score is *very* high. Our model is classifying training examples with near-perfect accuracy! \n\n<div class=\"alert alert-info\">\n    \n**Discussion questions**\n\nDo we trust our training scores to give a good prediction for model performance on unseen data? Have we built a good model?\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\">\n\nType your answer below.\n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"## Examining our model","metadata":{}},{"cell_type":"markdown","source":"Let's see how deep the fitted decision tree looks.","metadata":{}},{"cell_type":"code","source":"spotify_tree.get_depth()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">\n    \n**Discussion questions**\n\nYou should see a decision tree of depth 17. How many distinct training examples could a depth 17 decision tree memorize? How many examples exist in our dataset?\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\">\n\nType your answer below.\n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"It would be good to know what features our tree thought were important! Let's visualize the first few nodes.","metadata":{}},{"cell_type":"code","source":"# Training with just depth 3\n\nthree_model = DecisionTreeClassifier(max_depth=3, random_state=123)\nthree_model.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting\n\nplot_tree(three_model, filled=True, \n               feature_names = X_train.columns.to_list(),\n               impurity = False,\n               fontsize = 7,\n               label = None);","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"___\n<div class=\"alert alert-info\">\n    \n**Discussion questions**\n\nWhat features do you see the tree using? Do they match the features you expected to see based on the histograms in your EDA?\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\">\n\nType your answer below.\n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"_Type your answer here, replacing this text._","metadata":{}},{"cell_type":"markdown","source":"Our model achieved a near-perfect training score. However, since we trained on all data in our training set, we have not yet been able to measure model performance on new, unseen data.\n\nLet's check how well our trained model does on the test set.","metadata":{}},{"cell_type":"code","source":"spotify_tree.score(X_test, y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Hmmm. That wasn't nearly as good. We went from perfect predictions on training data to only **68% accuracy** on test data. It is likely that we have overfit.","metadata":{}},{"cell_type":"markdown","source":"## Finding a better model\n\nWe fit a very deep decision tree that likely just memorized our dataset. A shallower tree might limit some of the overfitting, so let's try forcing a maximum depth on our tree.","metadata":{}},{"cell_type":"code","source":"new_model = DecisionTreeClassifier(max_depth=5, random_state=123)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We train and score this model as usual on the training set.","metadata":{}},{"cell_type":"code","source":"new_model.fit(X_train, y_train)\nnew_model.score(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We get a lower training score than before. But what about test score?","metadata":{}},{"cell_type":"code","source":"new_model.score(X_test, y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There's still some overfitting, but we're getting a better test score than what we got before (with a deeper, more complex decision tree!) Also, there is less of a gap between our training and test scores, so our model is generalizating reasonably to new data, and making sensible predictions.","metadata":{}},{"cell_type":"markdown","source":"## Hold on... did we violate the Golden Rule?","metadata":{}},{"cell_type":"markdown","source":"We used our test set twice, which means we violated the Golden Rule of Machine Learning. In practice, it is **not** a good idea to use our test set multiple times with different models. The more often we use it, the less certain we can be of our model's predictions out in the real world. We might start overfitting to test data, even though we don't use it to directly update model parameters.\n\nThis is a real danger, and can lead us to picking bad models.","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">\n    \n**Discussion questions**\n\nIf re-using the test set is bad, but we cannot evaluate model performance until we measure it on unseen data, how are we to select good values for hyperparameters or choose between models?\n\nHow do we find the best `max_depth` for modelling our Spotify data?\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\">\n\nType your answer below.\n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"_Type your answer here, replacing this text._","metadata":{}},{"cell_type":"markdown","source":"Hyperparameter tuning is generally carried out using **cross-validation**. Essentially, this involves setting aside part of your training data as a 'validation set' to be used *only* for testing (and not for fitting each individual model). Then, validation accuracy can be used as a proxy for test accuracy.\n\nSklearn has many helper functions to make cross-validation easy (see [here](https://scikit-learn.org/1.5/modules/cross_validation.html#cross-validation-and-model-selection)).","metadata":{}},{"cell_type":"markdown","source":"## Feature Importance\n\nSklearn also has a built-in function to tell you which features the decision tree thinks are **most important** for making predictions (based on its learning).","metadata":{}},{"cell_type":"code","source":"impt = new_model.feature_importances_\nfor i, f in enumerate(X_train.columns.to_list()):\n    print(f\"{f}: \", round(impt[i], 4))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Food for thought... (Bonus Question)","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-info\">\n\n**Discussion questions**\n\nThe code below takes the \"index\" column of this dataset and allows us to use it as a feature. What might be the result of using the index as a feature for this dataset?\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\">\n\nType your answer below.\n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"_Type your answer here, replacing this text._","metadata":{}},{"cell_type":"code","source":"X_train_2 = X_train.reset_index()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"index_model = DecisionTreeClassifier(random_state=123)\nindex_model.fit(X_train_2, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_tree(index_model, impurity=False, filled=True);","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"That's a very simple tree... what happened?","metadata":{}}]}