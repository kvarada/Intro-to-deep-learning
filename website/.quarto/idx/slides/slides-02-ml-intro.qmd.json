{"title":"Introduction to Linear Models","markdown":{"yaml":{"title":"Introduction to Linear Models","format":{"revealjs":{"smaller":true,"center":true}}},"headingText":"Introduction to Machine Learning","containsRefs":false,"markdown":"\n\n\\\nMachine Learning uses computer programs to digest and accurately model data. After *training* on the data, a program can be used to extract hidden patterns, make predictions in new situations or generate novel content.\n\nThe program learns based on the *features* present in the data, which represent the information we have about each example.\n\n## Introduction to Machine Learning\n\\ \n\n![](img/sup-ML-terminology.png)\n\n\n## Activity 1\n\\\nWrite one (or several) problems in your area of work where you think Machine Learning could be applied. Try to address the following questions:\n\n* What goal are you trying to accomplish? What would an ideal solution to your problem look like?\n* How would a human solve this problem? What approaches are presently available and utilized?\n* What kind of data is available to you, or might be collected? What features are present in the data?\n\nOne of the learning objectives of the workshop will be to determine whether your goal is best addressed using supervised machine learning, inferential statistics, unsupervised learning, deep learning, generative AI, or a non-ML solution.\n\n## Classification vs. Regression\n\\\n\n![](img/classification-vs-regression.png)\n\n## Measuring Performance\n\\\n\n* Performance on classification tasks can be measured based on the *accuracy* of the model's predictions.\n\n* Performance on a regression task can be measured based on *error*. Mean squared error is one choice, but there are many others!\n\n\n## Inference vs. Prediction\n\\\n\n* *Inference* is the use of a model to infer a relationship between features (independent variables) and targets (independent variables).\n\n* *Prediction* is the use of a model to predict the target value for a new example not seen in training.\n\n\n## Example: Linear Regression\n\\\n![](img/visualization.png)\n\nIs this inference or prediction?\n\n## Types of Machine Learning\n\\\nThere are many kinds of machine learning, including\n\n* Supervised Learning (which we'll talk about a lot)\n\n* Unsupervised Learning\n\n* Semi-supervised learning\n\n* Reinforcement Learning\n\n* Generative AI\n\nWe will also discuss which problems each type might be best suited for.\n\n## Supervised Learning\n\\\nHere the training data is comprised of a set of *features*, and each example comes with a corresponding *target*. The goal is to get a machine learning model to accurately predict the target based on the feature values.\n\nExamples could include spam filtering, face recognition or weather forecasting.\n\nBy contrast, in unsupervised learning there are no targets. The goal is instead to uncover underlying patterns. These can be used to provide a concise summary of the data, or group similar examples together. Examples could include customer segmentation, anomaly detection or online recommendation systems (think Netflix).\n\n## Linear Classifiers\n\\\n\nWe can also build linear models for classification tasks. The idea is to convert the output from an arbitrary number to a number between 0 and 1, and treat it like a \"probability\".\n\nIn *logistic regression*, we squash the output using the sigmoid function and then adjust parameters (in training) to find the choice that makes the data \"most likely\".\n\n## Linear Classifiers\n\n![](img/us-map.png)\n\nCan you guess what this dataset is?\n\n## Linear Classifiers\n\n![](img/logistic.png)\n\nLogistic Regression predicts a *linear* decision boundary.\n\n## Sentiment Analysis: An Example\n\n```{python}\nimport sys, os\nsys.path.append(os.path.join(os.path.abspath(\".\"), \"code\"))\nfrom sup_learning import *\n```\n\\\nLet us attempt to use logistic regression to do sentiment analysis on a database of IMDB reviews. The database is available [here](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download).\n\n```{python}\n#| echo: True\nimdb_df = pd.read_csv(\"data/imdb_master.csv\", encoding=\"ISO-8859-1\")\nimdb_df.rename(columns={\"sentiment\": \"label\"}, inplace = True)\n```\n```{python}\nimdb_df = imdb_df[imdb_df[\"label\"].str.startswith((\"pos\", \"neg\"))]\nimdb_df.head()\n```\n\nWe will use only about 10% of the dataset for training (to speed things up)\n\n```{python}\nimdb_df[\"review_pp\"] = imdb_df[\"review\"].apply(replace_tags)\ntrain_df, test_df = train_test_split(imdb_df, test_size=0.9, random_state=123)\nX_train, y_train = train_df[\"review_pp\"], train_df[\"label\"]\nX_test, y_test = test_df[\"review_pp\"], test_df[\"label\"]\n```\n\n## Bag of Words\n\\\nTo create features that logistic regression can use, we will represent these reviews via a \"bag of words\" strategy.\n\nWe create a new feature for every word that appears in the dataset. Then, if a review contains that word exactly once, the corresponding feature gets a value of 1 for that review. If the word appears four times, the feature gets a value of 4. If the word is not present, it's marked as 0.\n\n## Bag of Words\n\\\nNotice that the result is a sparse matrix. Most reviews contain only a small number of words.\n\n```{python}\n#| echo: True\nvec = CountVectorizer(stop_words=\"english\")\nbow = vec.fit_transform(X_train)\nbow\n```\n\nThere are a total of 38867 \"words\" among the reviews. Here are some of them: \n\n```{python}\nvocab = vec.get_feature_names_out()\nvocab[::1000]\n```\n\n## Checking the class counts\n\nLet us see how many reviews are positive, and how many are negative.\n\\ \n\n```{python}\n#| echo: True\ny_train.value_counts()\n```\n\\\n\nThe dataset looks pretty balanced, so a classifier predicting at random would at best guess about 50% correctly.\n\nWe will not train our model.\n\n# Testing Performance\n\n## Testing Performance\n\\\n\nLet's see how the model performs after training.\n\n```{python}\npipe_lr = make_pipeline(\n    CountVectorizer(stop_words=\"english\"),\n    LogisticRegression(max_iter=1000),\n)\nscores = cross_validate(pipe_lr, X_train, y_train, return_train_score=True)\npd.DataFrame(scores)\n```\n\\\nWe're able to predict with roughly 84% accuracy on validation sets. Looks like our model learned something!\n\n## Tuning hyperparameters\n\\\n\nHowever, the training scores are perfect (and higher than validation scores) so our model is likely overfitting.\n\nMaybe it just memorized some rare words, each appearing only in one review, and associated these with the review's label. We could try reducing the size of our dictionary to prevent this.\n\n## Tuning hyperparameters\n\\\n\nThere are many tools available to automate the search for good hyperparameters. These can make our life easy, but there is always the danger of optimization bias in the results.\n\n\n```{python}\n\nfrom scipy.stats import loguniform, randint, uniform\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {\n    \"countvectorizer__max_features\": randint(10, len(vocab)),\n    \"logisticregression__C\": loguniform(1e-3, 1e3)\n}\npipe_lr = make_pipeline(CountVectorizer(stop_words=\"english\"), LogisticRegression(max_iter=1000))\nrandom_search = RandomizedSearchCV(pipe_lr, param_dist, n_iter=10, n_jobs=-1, return_train_score=True)\nrandom_search.fit(X_train, y_train)\n\nbest_model = random_search.best_estimator_\n\n```\n\n## Investigating the model\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\\\n\nLet's see what associations our model learned.\n\n```{python}\n#| code-overflow: scroll\n\n# Get feature names\nfeature_names = best_model.named_steps['countvectorizer'].get_feature_names_out().tolist()\n\n# Get coefficients \ncoeffs = best_model.named_steps[\"logisticregression\"].coef_.flatten()\n\nword_coeff_df = pd.DataFrame(coeffs, index=feature_names, columns=[\"Coefficient\"])\nword_coeff_df.sort_values(by=\"Coefficient\", ascending=False)\n```\n:::\n\n## Investigating the model\n\\\n\nThey make sense! Let's visualize the 20 most important features.\n\n```{python}\nmglearn.tools.visualize_coefficients(coeffs, feature_names, n_top_features=20)\n```\n\n## Making Predictions\n\\\n\nFinally, let's try predicting on some new examples.\n\\\n\n```{python}\nfake_reviews = [\"It got a bit boring at times but the direction was excellent and the acting was flawless. Overall I enjoyed the movie and I highly recommend it!\",\n \"The plot was shallower than a kiddie pool in a drought, but hey, at least we now know emojis should stick to texting and avoid the big screen.\"\n]\nfake_reviews\n```\n\nHere are the model predictions:\n\n```{python}\nbest_model.predict(fake_reviews)\n```\n\n\\\n\nLet's see which vocabulary words were present in the first review, and how they contributed to the classification.\n\n## Understanding Predictions\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n```{python}\nplot_coeff_example(best_model, fake_reviews[0], coeffs, feature_names)\n```\n:::\n\n\n## Summary\n\\\n\nThe bag-of-words representation was very simple-- we only counted which words appeared in which reviews. There was no attempt to maintain syntactical or grammatical structure or to study correlations between words.\n\nWe also trained on just 5000 examples. Nevertheless, our model performs quite well.\n\n\n\n## Linear Models\n\\\n\nPros:\n\n* Easy to train and to interpret\n* Widely applicable despite some strong assumptions\n* If you have a regression task, check whether a linear regression is already good enough! If you have a classification task, logistic regression is a go-to first option.\n\nCons:\n\n* Strong assumptions\n* Linear decision boundaries for classifiers\n* Correlated features can cause problems","srcMarkdownNoYaml":"\n\n## Introduction to Machine Learning\n\\\nMachine Learning uses computer programs to digest and accurately model data. After *training* on the data, a program can be used to extract hidden patterns, make predictions in new situations or generate novel content.\n\nThe program learns based on the *features* present in the data, which represent the information we have about each example.\n\n## Introduction to Machine Learning\n\\ \n\n![](img/sup-ML-terminology.png)\n\n\n## Activity 1\n\\\nWrite one (or several) problems in your area of work where you think Machine Learning could be applied. Try to address the following questions:\n\n* What goal are you trying to accomplish? What would an ideal solution to your problem look like?\n* How would a human solve this problem? What approaches are presently available and utilized?\n* What kind of data is available to you, or might be collected? What features are present in the data?\n\nOne of the learning objectives of the workshop will be to determine whether your goal is best addressed using supervised machine learning, inferential statistics, unsupervised learning, deep learning, generative AI, or a non-ML solution.\n\n## Classification vs. Regression\n\\\n\n![](img/classification-vs-regression.png)\n\n## Measuring Performance\n\\\n\n* Performance on classification tasks can be measured based on the *accuracy* of the model's predictions.\n\n* Performance on a regression task can be measured based on *error*. Mean squared error is one choice, but there are many others!\n\n\n## Inference vs. Prediction\n\\\n\n* *Inference* is the use of a model to infer a relationship between features (independent variables) and targets (independent variables).\n\n* *Prediction* is the use of a model to predict the target value for a new example not seen in training.\n\n\n## Example: Linear Regression\n\\\n![](img/visualization.png)\n\nIs this inference or prediction?\n\n## Types of Machine Learning\n\\\nThere are many kinds of machine learning, including\n\n* Supervised Learning (which we'll talk about a lot)\n\n* Unsupervised Learning\n\n* Semi-supervised learning\n\n* Reinforcement Learning\n\n* Generative AI\n\nWe will also discuss which problems each type might be best suited for.\n\n## Supervised Learning\n\\\nHere the training data is comprised of a set of *features*, and each example comes with a corresponding *target*. The goal is to get a machine learning model to accurately predict the target based on the feature values.\n\nExamples could include spam filtering, face recognition or weather forecasting.\n\nBy contrast, in unsupervised learning there are no targets. The goal is instead to uncover underlying patterns. These can be used to provide a concise summary of the data, or group similar examples together. Examples could include customer segmentation, anomaly detection or online recommendation systems (think Netflix).\n\n## Linear Classifiers\n\\\n\nWe can also build linear models for classification tasks. The idea is to convert the output from an arbitrary number to a number between 0 and 1, and treat it like a \"probability\".\n\nIn *logistic regression*, we squash the output using the sigmoid function and then adjust parameters (in training) to find the choice that makes the data \"most likely\".\n\n## Linear Classifiers\n\n![](img/us-map.png)\n\nCan you guess what this dataset is?\n\n## Linear Classifiers\n\n![](img/logistic.png)\n\nLogistic Regression predicts a *linear* decision boundary.\n\n## Sentiment Analysis: An Example\n\n```{python}\nimport sys, os\nsys.path.append(os.path.join(os.path.abspath(\".\"), \"code\"))\nfrom sup_learning import *\n```\n\\\nLet us attempt to use logistic regression to do sentiment analysis on a database of IMDB reviews. The database is available [here](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download).\n\n```{python}\n#| echo: True\nimdb_df = pd.read_csv(\"data/imdb_master.csv\", encoding=\"ISO-8859-1\")\nimdb_df.rename(columns={\"sentiment\": \"label\"}, inplace = True)\n```\n```{python}\nimdb_df = imdb_df[imdb_df[\"label\"].str.startswith((\"pos\", \"neg\"))]\nimdb_df.head()\n```\n\nWe will use only about 10% of the dataset for training (to speed things up)\n\n```{python}\nimdb_df[\"review_pp\"] = imdb_df[\"review\"].apply(replace_tags)\ntrain_df, test_df = train_test_split(imdb_df, test_size=0.9, random_state=123)\nX_train, y_train = train_df[\"review_pp\"], train_df[\"label\"]\nX_test, y_test = test_df[\"review_pp\"], test_df[\"label\"]\n```\n\n## Bag of Words\n\\\nTo create features that logistic regression can use, we will represent these reviews via a \"bag of words\" strategy.\n\nWe create a new feature for every word that appears in the dataset. Then, if a review contains that word exactly once, the corresponding feature gets a value of 1 for that review. If the word appears four times, the feature gets a value of 4. If the word is not present, it's marked as 0.\n\n## Bag of Words\n\\\nNotice that the result is a sparse matrix. Most reviews contain only a small number of words.\n\n```{python}\n#| echo: True\nvec = CountVectorizer(stop_words=\"english\")\nbow = vec.fit_transform(X_train)\nbow\n```\n\nThere are a total of 38867 \"words\" among the reviews. Here are some of them: \n\n```{python}\nvocab = vec.get_feature_names_out()\nvocab[::1000]\n```\n\n## Checking the class counts\n\nLet us see how many reviews are positive, and how many are negative.\n\\ \n\n```{python}\n#| echo: True\ny_train.value_counts()\n```\n\\\n\nThe dataset looks pretty balanced, so a classifier predicting at random would at best guess about 50% correctly.\n\nWe will not train our model.\n\n# Testing Performance\n\n## Testing Performance\n\\\n\nLet's see how the model performs after training.\n\n```{python}\npipe_lr = make_pipeline(\n    CountVectorizer(stop_words=\"english\"),\n    LogisticRegression(max_iter=1000),\n)\nscores = cross_validate(pipe_lr, X_train, y_train, return_train_score=True)\npd.DataFrame(scores)\n```\n\\\nWe're able to predict with roughly 84% accuracy on validation sets. Looks like our model learned something!\n\n## Tuning hyperparameters\n\\\n\nHowever, the training scores are perfect (and higher than validation scores) so our model is likely overfitting.\n\nMaybe it just memorized some rare words, each appearing only in one review, and associated these with the review's label. We could try reducing the size of our dictionary to prevent this.\n\n## Tuning hyperparameters\n\\\n\nThere are many tools available to automate the search for good hyperparameters. These can make our life easy, but there is always the danger of optimization bias in the results.\n\n\n```{python}\n\nfrom scipy.stats import loguniform, randint, uniform\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {\n    \"countvectorizer__max_features\": randint(10, len(vocab)),\n    \"logisticregression__C\": loguniform(1e-3, 1e3)\n}\npipe_lr = make_pipeline(CountVectorizer(stop_words=\"english\"), LogisticRegression(max_iter=1000))\nrandom_search = RandomizedSearchCV(pipe_lr, param_dist, n_iter=10, n_jobs=-1, return_train_score=True)\nrandom_search.fit(X_train, y_train)\n\nbest_model = random_search.best_estimator_\n\n```\n\n## Investigating the model\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\\\n\nLet's see what associations our model learned.\n\n```{python}\n#| code-overflow: scroll\n\n# Get feature names\nfeature_names = best_model.named_steps['countvectorizer'].get_feature_names_out().tolist()\n\n# Get coefficients \ncoeffs = best_model.named_steps[\"logisticregression\"].coef_.flatten()\n\nword_coeff_df = pd.DataFrame(coeffs, index=feature_names, columns=[\"Coefficient\"])\nword_coeff_df.sort_values(by=\"Coefficient\", ascending=False)\n```\n:::\n\n## Investigating the model\n\\\n\nThey make sense! Let's visualize the 20 most important features.\n\n```{python}\nmglearn.tools.visualize_coefficients(coeffs, feature_names, n_top_features=20)\n```\n\n## Making Predictions\n\\\n\nFinally, let's try predicting on some new examples.\n\\\n\n```{python}\nfake_reviews = [\"It got a bit boring at times but the direction was excellent and the acting was flawless. Overall I enjoyed the movie and I highly recommend it!\",\n \"The plot was shallower than a kiddie pool in a drought, but hey, at least we now know emojis should stick to texting and avoid the big screen.\"\n]\nfake_reviews\n```\n\nHere are the model predictions:\n\n```{python}\nbest_model.predict(fake_reviews)\n```\n\n\\\n\nLet's see which vocabulary words were present in the first review, and how they contributed to the classification.\n\n## Understanding Predictions\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n```{python}\nplot_coeff_example(best_model, fake_reviews[0], coeffs, feature_names)\n```\n:::\n\n\n## Summary\n\\\n\nThe bag-of-words representation was very simple-- we only counted which words appeared in which reviews. There was no attempt to maintain syntactical or grammatical structure or to study correlations between words.\n\nWe also trained on just 5000 examples. Nevertheless, our model performs quite well.\n\n\n\n## Linear Models\n\\\n\nPros:\n\n* Easy to train and to interpret\n* Widely applicable despite some strong assumptions\n* If you have a regression task, check whether a linear regression is already good enough! If you have a classification task, logistic regression is a go-to first option.\n\nCons:\n\n* Strong assumptions\n* Linear decision boundaries for classifiers\n* Correlated features can cause problems"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","output-file":"slides-02-ml-intro.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.5.57","auto-stretch":true,"editor":{"render-on-save":true},"publish":{"gh-pages":{"name":"gh-pages","branch":"main","path":"website"}},"title":"Introduction to Linear Models","smaller":true,"center":true}}},"projectFormats":["html"]}