{
  "hash": "573a640235e7399ddb1e1a5e9d56c7bf",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Introduction to Linear Models\"\nformat: \n    revealjs:\n      smaller: true\n      center: true\n---\n\n\n## Introduction to Machine Learning\n\\\nMachine Learning uses computer programs to digest and accurately model data. After *training* on the data, a program can be used to extract hidden patterns, make predictions in new situations or generate novel content.\n\nThe program learns based on the *features* present in the data, which represent the information we have about each example.\n\n## Introduction to Machine Learning\n\\ \n\n![](img/sup-ML-terminology.png)\n\n\n## Activity 1\n\\\nWrite one (or several) problems in your area of work where you think Machine Learning could be applied. Try to address the following questions:\n\n* What goal are you trying to accomplish? What would an ideal solution to your problem look like?\n* How would a human solve this problem? What approaches are presently available and utilized?\n* What kind of data is available to you, or might be collected? What features are present in the data?\n\nOne of the learning objectives of the workshop will be to determine whether your goal is best addressed using supervised machine learning, inferential statistics, unsupervised learning, deep learning, generative AI, or a non-ML solution.\n\n## Classification vs. Regression\n\\\n\n![](img/classification-vs-regression.png)\n\n## Measuring Performance\n\\\n\n* Performance on classification tasks can be measured based on the *accuracy* of the model's predictions.\n\n* Performance on a regression task can be measured based on *error*. Mean squared error is one choice, but there are many others!\n\n\n## Inference vs. Prediction\n\\\n\n* *Inference* is the use of a model to infer a relationship between features (independent variables) and targets (independent variables).\n\n* *Prediction* is the use of a model to predict the target value for a new example not seen in training.\n\n\n## Example: Linear Regression\n\\\n![](img/visualization.png)\n\nIs this inference or prediction?\n\n## Types of Machine Learning\n\\\nThere are many kinds of machine learning, including\n\n* Supervised Learning (which we'll talk about a lot)\n\n* Unsupervised Learning\n\n* Semi-supervised learning\n\n* Reinforcement Learning\n\n* Generative AI\n\nWe will also discuss which problems each type might be best suited for.\n\n## Supervised Learning\n\\\nHere the training data is comprised of a set of *features*, and each example comes with a corresponding *target*. The goal is to get a machine learning model to accurately predict the target based on the feature values.\n\nExamples could include spam filtering, face recognition or weather forecasting.\n\nBy contrast, in unsupervised learning there are no targets. The goal is instead to uncover underlying patterns. These can be used to provide a concise summary of the data, or group similar examples together. Examples could include customer segmentation, anomaly detection or online recommendation systems (think Netflix).\n\n## Linear Classifiers\n\\\n\nWe can also build linear models for classification tasks. The idea is to convert the output from an arbitrary number to a number between 0 and 1, and treat it like a \"probability\".\n\nIn *logistic regression*, we squash the output using the sigmoid function and then adjust parameters (in training) to find the choice that makes the data \"most likely\".\n\n## Linear Classifiers\n\n![](img/us-map.png)\n\nCan you guess what this dataset is?\n\n## Linear Classifiers\n\n![](img/logistic.png)\n\nLogistic Regression predicts a *linear* decision boundary.\n\n## Sentiment Analysis: An Example\n\n\n\n\\\nLet us attempt to use logistic regression to do sentiment analysis on a database of IMDB reviews. The database is available [here](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download).\n\n::: {#4c60c39d .cell execution_count=2}\n``` {.python .cell-code}\nimdb_df = pd.read_csv(\"data/imdb_master.csv\", encoding=\"ISO-8859-1\")\nimdb_df.rename(columns={\"sentiment\": \"label\"}, inplace = True)\n```\n:::\n\n\n::: {#85eaf24b .cell execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe will use only about 10% of the dataset for training (to speed things up)\n\n\n\n## Bag of Words\n\\\nTo create features that logistic regression can use, we will represent these reviews via a \"bag of words\" strategy.\n\nWe create a new feature for every word that appears in the dataset. Then, if a review contains that word exactly once, the corresponding feature gets a value of 1 for that review. If the word appears four times, the feature gets a value of 4. If the word is not present, it's marked as 0.\n\n## Bag of Words\n\\\nNotice that the result is a sparse matrix. Most reviews contain only a small number of words.\n\n::: {#3417234b .cell execution_count=5}\n``` {.python .cell-code}\nvec = CountVectorizer(stop_words=\"english\")\nbow = vec.fit_transform(X_train)\nbow\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<Compressed Sparse Row sparse matrix of dtype 'int64'\n\twith 439384 stored elements and shape (5000, 38867)>\n```\n:::\n:::\n\n\nThere are a total of 38867 \"words\" among the reviews. Here are some of them: \n\n::: {#1ed1235a .cell execution_count=6}\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\narray(['00', 'affection', 'apprehensive', 'barbara', 'blore',\n       'businessman', 'chatterjee', 'commanding', 'cramped', 'defining',\n       'displaced', 'edie', 'evolving', 'fingertips', 'gaffers',\n       'gravitas', 'heist', 'iliad', 'investment', 'kidnappee',\n       'licentious', 'mal√£', 'mice', 'museum', 'obsessiveness',\n       'parapsychologist', 'plasters', 'property', 'reclined',\n       'ridiculous', 'sayid', 'shivers', 'sohail', 'stomaches', 'syrupy',\n       'tolerance', 'unbidden', 'verneuil', 'wilcox'], dtype=object)\n```\n:::\n:::\n\n\n## Checking the class counts\n\nLet us see how many reviews are positive, and how many are negative.\n\\ \n\n::: {#fdb9ed6d .cell execution_count=7}\n``` {.python .cell-code}\ny_train.value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nlabel\npositive    2517\nnegative    2483\nName: count, dtype: int64\n```\n:::\n:::\n\n\n\\\n\nThe dataset looks pretty balanced, so a classifier predicting at random would at best guess about 50% correctly.\n\nWe will not train our model.\n\n# Testing Performance\n\n## Testing Performance\n\\\n\nLet's see how the model performs after training.\n\n::: {#51c5905d .cell execution_count=8}\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fit_time</th>\n      <th>score_time</th>\n      <th>test_score</th>\n      <th>train_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.425228</td>\n      <td>0.060137</td>\n      <td>0.828</td>\n      <td>0.99975</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.424452</td>\n      <td>0.062010</td>\n      <td>0.830</td>\n      <td>0.99975</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.427882</td>\n      <td>0.063532</td>\n      <td>0.848</td>\n      <td>0.99975</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.406247</td>\n      <td>0.058628</td>\n      <td>0.833</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.406079</td>\n      <td>0.060007</td>\n      <td>0.840</td>\n      <td>0.99975</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n\\\nWe're able to predict with roughly 84% accuracy on validation sets. Looks like our model learned something!\n\n## Tuning hyperparameters\n\\\n\nHowever, the training scores are perfect (and higher than validation scores) so our model is likely overfitting.\n\nMaybe it just memorized some rare words, each appearing only in one review, and associated these with the review's label. We could try reducing the size of our dictionary to prevent this.\n\n## Tuning hyperparameters\n\\\n\nThere are many tools available to automate the search for good hyperparameters. These can make our life easy, but there is always the danger of optimization bias in the results.\n\n\n\n## Investigating the model\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\\\n\nLet's see what associations our model learned.\n\n::: {#9db3cd82 .cell execution_count=10}\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Coefficient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>excellent</th>\n      <td>0.509825</td>\n    </tr>\n    <tr>\n      <th>great</th>\n      <td>0.459031</td>\n    </tr>\n    <tr>\n      <th>amazing</th>\n      <td>0.394692</td>\n    </tr>\n    <tr>\n      <th>best</th>\n      <td>0.385922</td>\n    </tr>\n    <tr>\n      <th>perfect</th>\n      <td>0.360205</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>terrible</th>\n      <td>-0.435527</td>\n    </tr>\n    <tr>\n      <th>boring</th>\n      <td>-0.466462</td>\n    </tr>\n    <tr>\n      <th>bad</th>\n      <td>-0.484084</td>\n    </tr>\n    <tr>\n      <th>awful</th>\n      <td>-0.530299</td>\n    </tr>\n    <tr>\n      <th>worst</th>\n      <td>-0.732084</td>\n    </tr>\n  </tbody>\n</table>\n<p>8700 rows √ó 1 columns</p>\n</div>\n```\n:::\n:::\n\n\n:::\n\n## Investigating the model\n\\\n\nThey make sense! Let's visualize the 20 most important features.\n\n::: {#7157aac6 .cell execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![](slides-02-ml-intro_files/figure-revealjs/cell-12-output-1.png){width=1236 height=436}\n:::\n:::\n\n\n## Making Predictions\n\\\n\nFinally, let's try predicting on some new examples.\n\\\n\n::: {#eb808637 .cell execution_count=12}\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n['It got a bit boring at times but the direction was excellent and the acting was flawless. Overall I enjoyed the movie and I highly recommend it!',\n 'The plot was shallower than a kiddie pool in a drought, but hey, at least we now know emojis should stick to texting and avoid the big screen.']\n```\n:::\n:::\n\n\nHere are the model predictions:\n\n::: {#7e20399e .cell execution_count=13}\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\narray(['positive', 'negative'], dtype=object)\n```\n:::\n:::\n\n\n\\\n\nLet's see which vocabulary words were present in the first review, and how they contributed to the classification.\n\n## Understanding Predictions\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#405948f0 .cell execution_count=14}\n\n::: {.cell-output .cell-output-stdout}\n```\nIt got a bit boring at times but the direction was excellent and the acting was flawless. Overall I enjoyed the movie and I highly recommend it!\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-02-ml-intro_files/figure-revealjs/cell-15-output-2.png){width=1223 height=428}\n:::\n:::\n\n\n:::\n\n\n## Summary\n\\\n\nThe bag-of-words representation was very simple-- we only counted which words appeared in which reviews. There was no attempt to maintain syntactical or grammatical structure or to study correlations between words.\n\nWe also trained on just 5000 examples. Nevertheless, our model performs quite well.\n\n\n\n## Linear Models\n\\\n\nPros:\n\n* Easy to train and to interpret\n* Widely applicable despite some strong assumptions\n* If you have a regression task, check whether a linear regression is already good enough! If you have a classification task, logistic regression is a go-to first option.\n\nCons:\n\n* Strong assumptions\n* Linear decision boundaries for classifiers\n* Correlated features can cause problems\n\n",
    "supporting": [
      "slides-02-ml-intro_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}