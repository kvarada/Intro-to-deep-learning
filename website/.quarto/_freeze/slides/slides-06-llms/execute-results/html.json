{
  "hash": "bd7d0ba49bda3413c5d7ad44d1c9e4ec",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Large Language Models\"\nformat: \n    revealjs:\n      smaller: true\n      center: true\n---\n\n\n## Learning outcomes \n\\\n\nBy the end of this module, you will be able to:\n\n- Explain language models and large language models\n- Describe the concept of self-attention\n- Distinguish between decoder-only, encoder-only, and encoder-decoder models\n- Apply pre-trained large language models for zero-shot learning\n\n\n\n## Take a guess\n\n- What do you think is the vocabulary size of young adult speakers of American English? \n\n\n## Language models activity \n\\\n\nEach of you will receive a sticky note with a word on it at some point. Here's what you'll do:\n\n- Carefully **remove the sticky note to see the word**. This word is for your eyes only ‚Äî- don't show it to your neighbours!\n- Think quickly: what word would logically follow the word on the sticky note? **Write this next word on a new sticky note**.\n- You have about 20 seconds for this step, so trust your instincts!\n- **Pass your predicted word to the person next to you**. Do not pass the word you **received** from your neighbour forward. Keep the chain going!\n- Stop after the last person in your row/table has finished. \n\n<br><br><br><br>\n\n## Markov model of language \n\\\n\n- You've just created a simple Markov model of language!\n- In predicting the next word from a minimal context, you likely used your linguistic intuition and familiarity with common two-word phrases or collocations.\n- You could create more coherent sentences by taking into account more context e.g., previous two words or four words or 100 words.\n\n## Language model \n- **A language model** computes the probability distribution over sequences (of words or characters). Intuitively, this probability tells us how \"good\" or plausible a sequence of words is. \n\n![](img/voice-assistant-ex.png)\n\n<!-- <img src=\"img/voice-assistant-ex.png\" height=\"1400\" width=\"1400\"> -->\n\nCheck out this [recent BMO ad](https://www.youtube.com/watch?v=VzqKtAYeJt4).\n\n\n## Smart compose \nA common application for predicting the next word is the 'smart compose' feature in your emails, text messages, and search engines.\n\n::: {#66cf301b .cell execution_count=2}\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n\n        <iframe\n            width=\"500\"\n            height=\"500\"\n            src=\"https://2.bp.blogspot.com/-KlBuhzV_oFw/WvxP_OAkJ1I/AAAAAAAACu0/T0F6lFZl-2QpS0O7VBMhf8wkUPvnRaPIACLcBGAs/s1600/image2.gif\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        \n```\n:::\n:::\n\n\n## Why should we care about predicting next word? \n\\\n\n- **Many practical language-related tasks can be cast as word prediction.**\n\n\n## Sentiment analysis as word prediction \n\n- We can cast sentiment analysis as language modeling by giving a language model a context like: \n\n> The sentiment of the sentence \"I like machine learning\" is: \n\n- And comparing the probability of word \"postive\" and the word negative. If the positive is more probable, we say the sentiment is positive, else negative. \n\n## Question answering as word prediction \n\n- We can cast question answering as language modeling by giving a language model a context like: \n\n> Q: Who won the Nobel Prize in 2024 for their work in deep learning? A: \n\n- We might expect to see that \"Geoffrey\" is very likely. If we continue and ask: \n\n> Q: Who won the Nobel Prize in 2024 for their work in deep learning? A: Geoffrey\n\n- We might expect to see that \"Hinton\" is very likely.\n\n\n## Text summarization casted as word prediction\n\n- Input: Long text such as a full length article\n- Output: Effective shorter summary of it\n- We can follow the text of the artile by a token like: **tl;dr; (too long; didn't read)**\n- Since this token is sufficiently common in the recent years, a lanaguage model have seen many texts in which this token occurs before a summary. So it will interpret it as an instruction to generate a summary. \n\n\n## A simple model of language \n\\\n\n- Calculate the co-occurrence frequencies and probabilities based on these frequencies\n- Predict the next word based on these probabilities\n- This is a Markov model of language. \n\n![](img/Markov-bigram-probs.png)\n\n\n## Long-distance dependencies \n\\\n\nWhat are some reasonable predictions for the next word in the sequence? \n\n> I am studying law at the University of British Columbia Point Grey campus in Vancouver because I want to work as a ___\n\nMarkov model is unable to capture such long-distance dependencies in language. \n\n## Transformer models\n\\\n\nEnter attention and transformer models! Transformer models are at the core of all state-of-the-art Generative AI models (e.g., BERT, GPT3, GPT4, Gemini, DALL-E, Llama, Github Copilot)? \n\n![](img/genai.png)\n\n[Source](https://jalammar.github.io/how-gpt3-works-visualizations-animations/)\n\n## Transformer models\n\\\n\n![](img/GPT-4-tech-report-abstract.png)\n\nSource: [GPT-4 Technical Report](https://arxiv.org/pdf/2303.08774.pdf)\n\n\n## Self-attention \n\\\n\n- An important innovation which makes these models work so well is **self-attention**. \n- Count how many times the players wearing the white pass the basketball?\n\n::: {#24e293d4 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n\n        <iframe\n            width=\"500\"\n            height=\"500\"\n            src=\"https://www.youtube.com/embed/vJG698U2Mvo\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        \n```\n:::\n:::\n\n\n## Self-attention \n\\\n\nWhen we process information, we often selectively focus on specific parts of the input, giving more attention to relevant information and less attention to irrelevant information. This is the core idea of **attention**.\n\nConsider the examples below: \n\n- Example 1: She left a brief **note** on the kitchen table, reminding him to pick up groceries.\n  \n- Example 2: The diplomat‚Äôs speech struck a positive **note** in the peace negotiations.\n\n- Example 3: She plucked the guitar strings, ending with a melancholic **note**.\n\nThe word **note** in these examples serves quite distinct meanings, each tied to different contexts. To capture varying word meanings across different contexts, we need a mechanism that considers the wider context to compute each word's contextual representation.\n\n- **Self-attention** is just that mechanism!\n\n\n## What is an LLM? \n\n- A **large language model** learns knowledge about language and the world from vast amounts of text. \n- It learns complexities of language simply by emersing in it without any text book to learn a language. \n- At a high level, training LLMs works as follows:  \n    - we feed the model batches of text\n    - it tries to predict what comes next\n    - we check the answers and based on how well it does, the model changes its internal settings (parameters)\n    - it's learning and improving   \n\n## Using LLMs in your applications\n\\\n\n- There are several Python libraries available which allow us to use pre-trained LLMs in our applications. \n    - [ü§ó Transformers library](https://huggingface.co/docs/transformers/index)\n    - [OpenAI GPT](https://pypi.org/project/openai/) \n    - [Haystack](https://pypi.org/project/farm-haystack/)\n    - [LangChain](https://python.langchain.com/v0.2/docs/introduction/)\n    - [`spacy-transformers`](https://spacy.io/universe/project/spacy-transformers)\n    - ...    \n\n## Types of LLMs \n\nIf you want to use pre-trained LLMs, it's useful to know that there are three main types of LLMs. \n\n| Feature   | Decoder-only (e.g., GPT-3)  | Encoder-only (e.g., BERT, RoBERTa) | Encoder decoder (e.g., T5, BARD)  |\n|-----------|-----------------------------|------------------------------------|-----------------------------------|\n| Output Computation is based on | Information earlier in the context | Entire context (bidirectional) | Encoded input context |\n| Text Generation     | Can naturally generate text completion | Cannot directly generate text| Can generate outputs naturally |\n| Example   | Our ML workshop audience is ___  | Our ML workshop audience is the best! ‚Üí positive                                     | Input: Translate to Mandarin: Long but productive day! Output:  Êº´ÈïøËÄåÂØåÊúâÊàêÊïàÁöÑ‰∏ÄÂ§©ÔºÅ  |\n\n\n## Pipelines before LLMs\n\\\n\n- Text preprocessing: Tokenization, stopword removal, stemming/lemmatization.\n- **Feature extraction**: Bag of Words or word embeddings.\n- Training: Supervised learning on a labeled dataset (e.g., with positive, negative, and neutral sentiment categories for sentiment analysis).\n- Evaluation: Performance typically measured using accuracy, F1-score, etc.\n- **Main challenges:**\n    - Extensive feature engineering required for good performance.\n    - Difficulty in capturing the nuances and context of sentiment, especially in complex sentences.\n\n## Pipelines after LLMs\n\\ \n\n::: {#16ef2902 .cell execution_count=4}\n``` {.python .cell-code}\nfrom transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n# Sentiment analysis pipeline\nanalyzer = pipeline(\"sentiment-analysis\", model='distilbert-base-uncased-finetuned-sst-2-english')\nanalyzer([\"I asked my model to predict my future, and it said '404: Life not found.'\",\n          '''Machine learning is just like cooking‚Äîsometimes you follow the recipe, \n            and other times you just hope for the best!.'''])\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n[{'label': 'NEGATIVE', 'score': 0.995707631111145},\n {'label': 'POSITIVE', 'score': 0.9994770884513855}]\n```\n:::\n:::\n\n\n## Zero-shot learning \n\\\n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#afb0a4e3 .cell execution_count=5}\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n['i left with my bouquet of red and yellow tulips under my arm feeling slightly more optimistic than when i arrived',\n 'i was feeling a little vain when i did this one',\n 'i cant walk into a shop anywhere where i do not feel uncomfortable',\n 'i felt anger when at the end of a telephone call',\n 'i explain why i clung to a relationship with a boy who was in many ways immature and uncommitted despite the excitement i should have been feeling for getting accepted into the masters program at the university of virginia',\n 'i like to have the same breathless feeling as a reader eager to see what will happen next',\n 'i jest i feel grumpy tired and pre menstrual which i probably am but then again its only been a week and im about as fit as a walrus on vacation for the summer',\n 'i don t feel particularly agitated',\n 'i feel beautifully emotional knowing that these women of whom i knew just a handful were holding me and my baba on our journey',\n 'i pay attention it deepens into a feeling of being invaded and helpless',\n 'i just feel extremely comfortable with the group of people that i dont even need to hide myself',\n 'i find myself in the odd position of feeling supportive of']\n```\n:::\n:::\n\n\n:::\n\n## Zero-shot learning for emotion detection\n\\\n\n::: {#945b7a9e .cell execution_count=6}\n``` {.python .cell-code}\nfrom transformers import AutoTokenizer\nfrom transformers import pipeline \nimport torch\n\n#Load the pretrained model\nmodel_name = \"facebook/bart-large-mnli\"\nclassifier = pipeline('zero-shot-classification', model=model_name)\nexs = dataset[\"test\"][\"text\"][:10]\ncandidate_labels = [\"sadness\", \"joy\", \"love\",\"anger\", \"fear\", \"surprise\"]\noutputs = classifier(exs, candidate_labels)\n```\n:::\n\n\n## Zero-shot learning for emotion detection\n\\\n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#49d36b86 .cell execution_count=7}\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sequence</th>\n      <th>labels</th>\n      <th>scores</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>im feeling rather rotten so im not very ambiti...</td>\n      <td>[sadness, anger, surprise, fear, joy, love]</td>\n      <td>[0.7367963194847107, 0.10041721910238266, 0.09...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>im updating my blog because i feel shitty</td>\n      <td>[sadness, surprise, anger, fear, joy, love]</td>\n      <td>[0.7429746985435486, 0.13775986433029175, 0.05...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i never make her separate from me because i do...</td>\n      <td>[love, sadness, surprise, fear, anger, joy]</td>\n      <td>[0.3153638243675232, 0.22490324079990387, 0.19...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i left with my bouquet of red and yellow tulip...</td>\n      <td>[surprise, joy, love, sadness, fear, anger]</td>\n      <td>[0.42182087898254395, 0.3336702883243561, 0.21...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i was feeling a little vain when i did this one</td>\n      <td>[surprise, anger, fear, love, joy, sadness]</td>\n      <td>[0.5639430284500122, 0.17000176012516022, 0.08...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>i cant walk into a shop anywhere where i do no...</td>\n      <td>[surprise, fear, sadness, anger, joy, love]</td>\n      <td>[0.37033382058143616, 0.36559492349624634, 0.1...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>i felt anger when at the end of a telephone call</td>\n      <td>[anger, surprise, fear, sadness, joy, love]</td>\n      <td>[0.9760521054267883, 0.01253431849181652, 0.00...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>i explain why i clung to a relationship with a...</td>\n      <td>[surprise, joy, love, sadness, fear, anger]</td>\n      <td>[0.4382022023200989, 0.232231006026268, 0.1298...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>i like to have the same breathless feeling as ...</td>\n      <td>[surprise, joy, love, fear, anger, sadness]</td>\n      <td>[0.7675782442092896, 0.13846899569034576, 0.03...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>i jest i feel grumpy tired and pre menstrual w...</td>\n      <td>[surprise, sadness, anger, fear, joy, love]</td>\n      <td>[0.7340186834335327, 0.11860235780477524, 0.07...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n\n## Fun tools \n- [NotebookLM](https://notebooklm.google/) \n- [LIDA](https://microsoft.github.io/lida/)\n- [LangChain](https://www.langchain.com/)\n- [Graphologue](https://graphologue.app/)\n- ...\n\n## Harms of large language models \n\nWhile these models are super powerful and useful, be mindful of the harms caused by these models. Some of the harms as summarized [here](https://stanford-cs324.github.io/winter2022/lectures/harms-1/) are: \n\n- performance disparties\n- social biases and stereotypes\n- toxicity\n- misinformation\n- security and privacy risks\n- copyright and legal protections\n- environmental impact\n- centralization of power\n\n## Thank you! \n\n- That's it for the modules! Now, let's work on exercises. \n\n",
    "supporting": [
      "slides-06-llms_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}