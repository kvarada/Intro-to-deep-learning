{
  "hash": "83eeb5f2f93c708e5292fa979b8b07c0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Supervised Machine Learning Models\"\nformat: \n    revealjs:\n      smaller: true\n      center: true\n---\n\n\n## Supervised Learning\n\\\n\nIn the next section, we will briefly introduce a few types of machine learning models that are often used for supervised learning tasks.\n\nWe will discuss some basic intuition around how they work, and also discuss their relative strengths and shortcomings.\n\n# Tree-based models\n\n## Tree-based models\n\n![](img/actual-tree.jpg)\n\nWe have seen that decision trees are prone to overfitting. There are several models that extend the basic idea of using decision trees.\n\n## Random Forest\n\n![](img/random-forest.png)\n\nTrain an ensemble of distinct decision trees.\n\n## Random Forest\n\\\n\nEach tree trains on a random sample of the data. Some times the features used to split are also randomized at each node.\n\nIdea: Individual trees still learn noise in the data, but the noise should \"average out\" over the ensemble.\n\n## Gradient Boosted Trees\n\n![](img/boost.png)\n\nEach tree tries to \"correct\" or improve the previous tree's prediction.\n\n## Tree-Based Models\n\\ \n\nRandom Forest, XGBoost, etc are all easily available as \"out-of-the box solutions\".\n\nPros: \n\n* Perform well on a variety of tasks\n* Random forest in particular are easy to train and robust to outliers.\n\nCons:\n\n* Not always interpretable\n* Not good at handling sparse data\n* Can also still overfit.\n\n\n\n# Linear models\n\n## Linear models\n\\\n\n![](img/lse)\n\nMany of you might be familiar with least-squares regression. We find the line of best fit by minimizing the 'squared error' of the predictions.\n\n\n## Linear Models\n\\ \n![](img/outlierbad.png)\n\nSquared Error is very sensitive to outliers. Far-away points contribute a very large squared error, and even relatively few points can affect the outcome.\n\n## Linear Models\n\\ \n![](img/outliergood.png)\n\nWe can use other notions of \"best fit\". Using absolute error makes the model more resistant to outliers!\n\n## Linear Classifiers\n\\\n\nWe can also build linear models for classification tasks. The idea is to convert the output from an arbitrary number to a number between 0 and 1, and treat it like a \"probability\".\n\nIn *logistic regression*, we squash the output using the sigmoid function and then adjust parameters (in training) to find the choice that makes the data \"most likely\".\n\n## Linear Classifiers\n\n![](img/us-map.png)\n\nCan you guess what this dataset is?\n\n## Linear Classifiers\n\n![](img/logistic.png)\n\nLogistic Regression predicts a *linear* decision boundary.\n\n## Sentiment Analysis: An Example\n\n\n\n\\\nLet us attempt to use logistic regression to do sentiment analysis on a database of IMDB reviews. The database is available [here](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download).\n\n::: {#220ceefd .cell execution_count=2}\n``` {.python .cell-code}\nimdb_df = pd.read_csv(\"data/imdb_master.csv\", encoding=\"ISO-8859-1\")\nimdb_df.rename(columns={\"sentiment\": \"label\"}, inplace = True)\n```\n:::\n\n\n::: {#73afe7d8 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe will use only about 10% of the dataset for training (to speed things up)\n\n\n\n## Bag of Words\n\\\nTo create features that logistic regression can use, we will represent these reviews via a \"bag of words\" strategy.\n\nWe create a new feature for every word that appears in the dataset. Then, if a review contains that word exactly once, the corresponding feature gets a value of 1 for that review. If the word appears four times, the feature gets a value of 4. If the word is not present, it's marked as 0.\n\n## Bag of Words\n\\\nNotice that the result is a sparse matrix. Most reviews contain only a small number of words.\n\n::: {#940dbdcb .cell execution_count=5}\n``` {.python .cell-code}\nvec = CountVectorizer(stop_words=\"english\")\nbow = vec.fit_transform(X_train)\nbow\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<Compressed Sparse Row sparse matrix of dtype 'int64'\n\twith 439384 stored elements and shape (5000, 38867)>\n```\n:::\n:::\n\n\nThere are a total of 38867 \"words\" among the reviews. Here are some of them: \n\n::: {#f32f6917 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\narray(['00', 'affection', 'apprehensive', 'barbara', 'blore',\n       'businessman', 'chatterjee', 'commanding', 'cramped', 'defining',\n       'displaced', 'edie', 'evolving', 'fingertips', 'gaffers',\n       'gravitas', 'heist', 'iliad', 'investment', 'kidnappee',\n       'licentious', 'malã', 'mice', 'museum', 'obsessiveness',\n       'parapsychologist', 'plasters', 'property', 'reclined',\n       'ridiculous', 'sayid', 'shivers', 'sohail', 'stomaches', 'syrupy',\n       'tolerance', 'unbidden', 'verneuil', 'wilcox'], dtype=object)\n```\n:::\n:::\n\n\n## Checking the class counts\n\nLet us see how many reviews are positive, and how many are negative.\n\\ \n\n::: {#02f3ea82 .cell execution_count=7}\n``` {.python .cell-code}\ny_train.value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nlabel\npositive    2517\nnegative    2483\nName: count, dtype: int64\n```\n:::\n:::\n\n\n\\\n\nThe dataset looks pretty balanced, so a classifier predicting at random would at best guess about 50% correctly.\n\nWe will not train our model.\n\n# Testing Performance\n\n## Testing Performance\n\\\n\nLet's see how the model performs after training.\n\n::: {#fe149da6 .cell execution_count=8}\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fit_time</th>\n      <th>score_time</th>\n      <th>test_score</th>\n      <th>train_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.411476</td>\n      <td>0.058247</td>\n      <td>0.828</td>\n      <td>0.99975</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.407961</td>\n      <td>0.059842</td>\n      <td>0.830</td>\n      <td>0.99975</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.422086</td>\n      <td>0.058663</td>\n      <td>0.848</td>\n      <td>0.99975</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.408087</td>\n      <td>0.057880</td>\n      <td>0.833</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.397947</td>\n      <td>0.059378</td>\n      <td>0.840</td>\n      <td>0.99975</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n\\\nWe're able to predict with roughly 84% accuracy on validation sets. Looks like our model learned something!\n\n## Tuning hyperparameters\n\\\n\nHowever, the training scores are perfect (and higher than validation scores) so our model is likely overfitting.\n\nMaybe it just memorized some rare words, each appearing only in one review, and associated these with the review's label. We could try reducing the size of our dictionary to prevent this.\n\n## Tuning hyperparameters\n\\\n\nThere are many tools available to automate the search for good hyperparameters. These can make our life easy, but there is always the danger of optimization bias in the results.\n\n\n\n## Investigating the model\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\\\n\nLet's see what associations our model learned.\n\n::: {#6039ffdf .cell execution_count=10}\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Coefficient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>excellent</th>\n      <td>0.792911</td>\n    </tr>\n    <tr>\n      <th>perfect</th>\n      <td>0.608851</td>\n    </tr>\n    <tr>\n      <th>amazing</th>\n      <td>0.602716</td>\n    </tr>\n    <tr>\n      <th>wonderful</th>\n      <td>0.564188</td>\n    </tr>\n    <tr>\n      <th>surprised</th>\n      <td>0.536449</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>waste</th>\n      <td>-0.669122</td>\n    </tr>\n    <tr>\n      <th>terrible</th>\n      <td>-0.697831</td>\n    </tr>\n    <tr>\n      <th>boring</th>\n      <td>-0.709981</td>\n    </tr>\n    <tr>\n      <th>awful</th>\n      <td>-0.870623</td>\n    </tr>\n    <tr>\n      <th>worst</th>\n      <td>-1.117365</td>\n    </tr>\n  </tbody>\n</table>\n<p>5775 rows × 1 columns</p>\n</div>\n```\n:::\n:::\n\n\n:::\n\n## Investigating the model\n\\\n\nThey make sense! Let's visualize the 20 most important features.\n\n::: {#fdc6f40c .cell execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![](slides-03-ml-models_files/figure-revealjs/cell-12-output-1.png){width=1223 height=489}\n:::\n:::\n\n\n## Making Predictions\n\\\n\nFinally, let's try predicting on some new examples.\n\\\n\n::: {#b7ea4de7 .cell execution_count=12}\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n['It got a bit boring at times but the direction was excellent and the acting was flawless. Overall I enjoyed the movie and I highly recommend it!',\n 'The plot was shallower than a kiddie pool in a drought, but hey, at least we now know emojis should stick to texting and avoid the big screen.']\n```\n:::\n:::\n\n\nHere are the model predictions:\n\n::: {#07b7af7d .cell execution_count=13}\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\narray(['positive', 'negative'], dtype=object)\n```\n:::\n:::\n\n\n\\\n\nLet's see which vocabulary words were present in the first review, and how they contributed to the classification.\n\n## Understanding Predictions\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#fc00820f .cell execution_count=14}\n\n::: {.cell-output .cell-output-stdout}\n```\nIt got a bit boring at times but the direction was excellent and the acting was flawless. Overall I enjoyed the movie and I highly recommend it!\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-ml-models_files/figure-revealjs/cell-15-output-2.png){width=1223 height=428}\n:::\n:::\n\n\n:::\n\n\n## Summary\n\\\n\nThe bag-of-words representation was very simple-- we only counted which words appeared in which reviews. There was no attempt to maintain syntactical or grammatical structure or to study correlations between words.\n\nWe also trained on just 5000 examples. Nevertheless, our model performs quite well.\n\n\n\n## Linear Models\n\\\n\nPros:\n\n* Easy to train and to interpret\n* Widely applicable despite some strong assumptions\n* If you have a regression task, check whether a linear regression is already good enough! If you have a classification task, logistic regression is a go-to first option.\n\nCons:\n\n* Strong assumptions\n* Linear decision boundaries for classifiers\n* Correlated features can cause problems\n\n# (Optional) Analogy-based models\n\n## Analogy-based models\n\n![](img/us-map.png)\n\nReturning to our older dataset.\n\n## Analogy-based models\n\n![](img/us-map-dot.jpg)\n\nHow would you classify the green dot?\n\n## Analogy-based models\n\\\n\nIdea: predict on new data based on \"similar\" examples in the training data.\n\n## *K*-Nearest-Neighbour Classifier\n\\\n\nFind the *K* nearest neighbours of an example, and predict whichever class was most common among them.\n\n'*K*' is a hyperparameter. Choosing *K=1* is likely to overfit. If the dataset has *N* examples, setting *K=N* just predicts the mode (dummy classifier).\n\nNo training phase, but the model can get arbitrarily large (and take very long to make predictions).\n\n## SVM with RBF kernel\n\\ \n\nAnother 'analogy-based' classification method.\n\nThe model stores examples with positive and negative weights. Being close to a positive example makes your label more likely to be positive.\n\nCan lead to \"smoother\" decision boundaries than K-NNs, and potentially to a smaller trained model.\n\n## KNNs and SVMs\n\\ \n![](img/knn-vs-svm.png)\n\n## Analogy-based Models\n\\\n\nPros:\n\n* Do not need to make assumptions about the underlying data\n* Given enough data, should pretty much always work.\n\nCons:\n\n* *Enough* data can mean ... a lot\n* Computing distances is time-consuming for large datasets\n* Can't really interpret the model's decisions.\n\n## A Look Ahead\n\\\n\nSupport Vector Machines (SVM) are also linear classifiers.\n\nThe reason we see a *non-linear* decision boundary is the use of the *RBF* kernel, which applies a certain non-linear transformation to the features.\n\nEven if our data is not linearly separable, there could be a good choice of feature transform out there that *makes* it linearly separable.\n\n##\n\nWouldn't it be nice if we could train a machine learning model to find such a transform?\n\n",
    "supporting": [
      "slides-03-ml-models_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}