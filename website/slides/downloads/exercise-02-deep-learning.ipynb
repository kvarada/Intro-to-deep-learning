{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"name":"lab4.ipynb","provenance":[],"version":"0.3.2"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":30378,"sourceType":"datasetVersion","datasetId":23777},{"sourceId":4292212,"sourceType":"datasetVersion","datasetId":2529046}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"otter":{"OK_FORMAT":true,"tests":{}},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"vscode":{"interpreter":{"hash":"f821000d0c0da66e5bcde88c37d59c8e0de03b40667fb62009a8148ca49465a0"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exercise 2: Transfer Learning","metadata":{},"attachments":{}},{"cell_type":"markdown","source":"Attribution: Kolhatkar, Varada (2024) [DSCI 572](https://ubc-mds.github.io/DSCI_572_sup-learn-2/README.html) ","metadata":{}},{"cell_type":"markdown","source":"**Transfer learning** is like borrowing knowledge from one task to help with another: you take a model that has already learned patterns from a related task (e.g., classifying images in [Imagenet](https://www.image-net.org/)) and adapt it to your task (e.g., detecting specific types of fruits) with less effort and data. \n\nIn this exercise, you will explore transfer learning by leveraging pre-trained image classification models. Specifically, you will:\n\n- Use these models out of the box to classify your own images.\n  \n- Use them as feature extractors to obtain rich representations of your images, which you can then apply to your own tasks.","metadata":{}},{"cell_type":"markdown","source":"**Important!!**\n\nWe are going to run this notebook on the cloud using [Kaggle](https://www.kaggle.com). Kaggle offers **30 hours** of free GPU usage per week which should be much more than enough for this lab. \n\nYou should make sure the followings are ready **before** you start the lab.\n\n- Create an Kaggle account [here](https://www.kaggle.com/) if you don't have one yet\n  \n- Verify your phone number [here](https://www.kaggle.com/settings) to get access to GPUs","metadata":{}},{"cell_type":"markdown","source":"## Getting Started with Kaggle Kernels\n<hr>","metadata":{}},{"cell_type":"markdown","source":"To get started, follow these steps:\n\n1. Go to https://www.kaggle.com/kernels\n2. Select `+ New Notebook`\n3. Click `File` on the top left side of your Kaggle notebook, select `Import Notebook`\n4. Upload this notebook\n5. On the right-hand side of your Kaggle notebook, find `Session options` and make sure:\n  \n  - `INTERNET` is enabled.\n  \n  - In the `ACCELERATOR` dropdown, choose the options starts with `GPU` when you're ready to use it (you can turn it on/off as you need it).\n    \n7. **Run** the cell below for preparation the model, labels and functions.\n> The code in the following cell contains helper functions that will be used later. You don't need to fully understand this code to answer the questions in this notebook.","metadata":{"colab_type":"text","id":"91yxVPKkO2qa"},"attachments":{}},{"cell_type":"code","source":"# Import \nfrom PIL import Image\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom torch import nn, optim\nfrom torchvision import datasets, models, transforms, utils\nimport glob\nimport json\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os, sys\nimport pandas as pd\nimport random\nimport torch\nimport torchvision\n%matplotlib inline\n\nplt.rcParams.update({'axes.grid': False})\n\n# Download ImageNet labels\n!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n\n# Prepare the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize model globally for reuse\nvgg_model = models.get_model('vgg16', weights='VGG16_Weights.DEFAULT').to(device)\nvgg_model.eval()\n\ndensenet_model = models.get_model('densenet121', weights=\"DenseNet121_Weights.IMAGENET1K_V1\").to(device)\ndensenet_model.classifier = nn.Identity()  # remove that last \"classification\" layer\ndensenet_model.eval()\n\ndef classify_image(img: Image.Image, topn: int = 4) -> pd.DataFrame:\n    \"\"\"\n    Classify an image using a pre-trained VGG16 model.\n    \n    Args:\n        img: PIL Image to classify\n        topn: Number of top predictions to return\n        \n    Returns:\n        DataFrame with top class predictions and their probabilities\n    \"\"\"\n    preprocess = transforms.Compose([\n                transforms.Resize(299),\n                transforms.CenterCrop(299),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                                  std=[0.229, 0.224, 0.225])])\n\n    with open(\"imagenet_classes.txt\", \"r\") as f:\n        classes = [line.strip() for line in f.readlines()]\n    \n    img_t = preprocess(img)\n    batch_t = torch.unsqueeze(img_t, 0).to(device)\n    \n    with torch.no_grad():\n        output = vgg_model(batch_t)\n        _, indices = torch.sort(output, descending=True)\n        probabilities = torch.nn.functional.softmax(output, dim=1)\n    \n    d = {'Predicted Class': [classes[idx] for idx in indices[0][:topn]], \n         'Probability Score': [np.round(probabilities[0, idx].item(),3) for idx in indices[0][:topn]]}\n    return pd.DataFrame(d, columns=['Predicted Class','Probability Score'])\n\n\n# Attribution: [Code from PyTorch docs](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html?highlight=transfer%20learning)\nIMAGE_SIZE = 200\nBATCH_SIZE = 64\n\ndef read_data(data_dir: str, subdir: dict) -> tuple:\n    \"\"\"\n    Reads and preprocesses image data from directories.\n    \n    Args:\n        data_dir: Base directory containing image data\n        subdir: Dictionary with train/valid subdirectories\n        \n    Returns:\n        tuple: (image_datasets, dataloaders) containing the processed datasets\n    \"\"\"\n    data_transforms = {\n        \"train\": transforms.Compose(\n            [\n                transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),     \n                transforms.ToTensor(),\n                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),            \n            ]\n        ),\n        \"valid\": transforms.Compose(\n            [\n                transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),                        \n                transforms.ToTensor(),\n                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),                        \n            ]\n        ),\n    }\n\n    image_datasets = {\n        x: datasets.ImageFolder(os.path.join(data_dir, subdir[x]), data_transforms[x])\n        for x in [\"train\", \"valid\"]\n    }\n    \n    dataloaders = {}\n    \n    dataloaders[\"train\"] = torch.utils.data.DataLoader(\n            image_datasets[\"train\"], batch_size=BATCH_SIZE, shuffle=True\n        )\n    \n    dataloaders[\"valid\"] = torch.utils.data.DataLoader(\n            image_datasets[\"valid\"], batch_size=BATCH_SIZE, shuffle=True\n        )\n    \n    return image_datasets, dataloaders\n\ndef get_features(model: nn.Module, data_loader: torch.utils.data.DataLoader, seed: int = None, verbose: bool = False) -> tuple:\n    \"\"\"\n    Extract features from penultimate layer of model.\n    \n    Args:\n        model: Pre-trained neural network\n        data_loader: DataLoader containing images\n        seed: Random seed for reproducibility\n        \n    Returns:\n        tuple: (features, labels) as torch tensors\n    \"\"\"\n    if seed:\n        torch.manual_seed(seed)\n    model.to(device)\n    with torch.no_grad():  # turn off computational graph stuff\n        Z_init = torch.empty((0, 1024)).to(device)  # Initialize empty tensors\n        y_init = torch.empty((0)).to(device)\n        for X, y in data_loader:\n            X, y = X.to(device), y.to(device)\n            Z_init = torch.cat((Z_init, model(X)), dim=0)\n            y_init = torch.cat((y_init, y))\n    if verbose:\n        print(f'Sample feature vectors: \\n {pd.DataFrame(Z_init.cpu().detach())[:10]} \\n')\n    return Z_init.cpu().detach(), y_init.cpu().detach()\n\ndef show_predictions(pipe, \n                    Z_valid: torch.Tensor,\n                    y_valid: torch.Tensor, \n                    dataloader: torch.utils.data.DataLoader,\n                    class_names: dict,\n                    num_images: int = 20,\n                    seed: int = None) -> None:\n    \"\"\"\n    Display images with predicted and true labels.\n    \n    Args:\n        pipe: Trained sklearn pipeline\n        Z_valid: Validation features\n        y_valid: Validation labels  \n        dataloader: DataLoader for images\n        class_names: Dictionary mapping indices to class names\n        num_images: Number of images to display\n        seed: Random seed for reproducibility\n    \"\"\"\n    if seed:\n        torch.manual_seed(seed)\n    images_so_far = 0\n    fig = plt.figure(figsize=(15, 25))  # Adjust the figure size for better visualization\n\n    # Convert the features and labels to numpy arrays\n    Z_valid = Z_valid.numpy()\n    y_valid = y_valid.numpy()\n\n    # Make predictions using the trained logistic regression model\n    preds = pipe.predict(Z_valid)\n\n    with torch.no_grad():\n        for idx, (inputs, labels) in enumerate(dataloader):\n            inputs = inputs.cpu()\n            for j in range(inputs.size()[0]):\n                if images_so_far >= num_images:\n                    return\n                # print(f\"Dataloader Labels: {labels[j]}: {class_names['valid'][labels[j]]}\")\n                ax = plt.subplot(num_images // 5, 5, images_so_far + 1)  # 5 images per row\n                ax.axis('off')\n                # A manual hotfix for the wrong directory naming in the dataset\n                ax.set_title(f\"Predicted Class: {class_names['train'][int(preds[images_so_far])]} \\n\"\n                             f\"Actual Label: {class_names['valid'][int(y_valid[images_so_far])]}\")\n                inp = inputs.data[j].numpy().transpose((1, 2, 0))\n                mean = np.array([0.5, 0.5, 0.5])\n                std = np.array([0.5, 0.5, 0.5])\n                inp = std * inp + mean\n                inp = np.clip(inp, 0, 1)\n                ax.imshow(inp)\n                #imshow(inputs.data[j])\n                images_so_far += 1\n\ndef show_image_label_prob(image_path: str, true_label: str, num_images: int) -> None:\n    \"\"\"\n    Display images with classifications and probabilities.\n    \n    Args:\n        image_path: Glob pattern to match image files\n        true_label: Actual label of the images\n        num_images: Number of random images to display\n    \"\"\"\n    images = glob.glob(image_path)\n    selected_images = random.sample(images, num_images)\n    plt.figure(figsize=(5, 5));\n    for image in selected_images:\n        img = Image.open(image)\n        img.load()\n        plt.imshow(img)\n        plt.title(f'Actual Label: {true_label}')\n        plt.show()\n        df = classify_image(img)    \n        print(df.to_string(index=False))\n        print(\"--------------------------------------------------------------\\n\\n\")\n\ndef extract_features_and_train_classifier(DATA_DIR: str, SUBDIR: dict, seed: int = None, verbose: bool = True):\n    \"\"\"\n    Extract features and train a classifier on image data.\n    \n    Args:\n        DATA_DIR: Base directory containing image data\n        SUBDIR: Dictionary with train/valid subdirectories\n        seed: Random seed for reproducibility\n        verbose: Whether to print training progress\n        \n    Returns:\n        Dictionary containing model, features, and class names\n    \"\"\"\n    image_datasets, dataloaders = read_data(DATA_DIR, SUBDIR)\n    class_names = {\n        \"train\": image_datasets[\"train\"].classes,\n        \"valid\": image_datasets[\"valid\"].classes\n    }\n    \n    Z_train, y_train = get_features(\n        densenet_model, dataloaders[\"train\"], \n        seed=seed, verbose=verbose\n    )\n    Z_valid, y_valid = get_features(\n        densenet_model, dataloaders[\"valid\"],\n        seed=seed\n    )\n    \n    pipe = make_pipeline(StandardScaler(), LogisticRegression(max_iter=2000))\n    \n    if verbose:\n        print(\"======== ML Model Training In Progress ========\")\n    \n    pipe.fit(Z_train, y_train)\n    \n    if verbose:\n        print(f\"Training score: {pipe.score(Z_train, y_train):.3f}\")\n        print(f\"Validation score: {pipe.score(Z_valid, y_valid):.3f}\")\n        print(\"======== ML Model Training Complete ========\")\n\n    return {\n        \"class_names\": class_names,\n        \"dataloaders\": dataloaders,\n        \"Z_train\": Z_train,\n        \"y_train\": y_train,\n        \"Z_valid\": Z_valid,\n        \"y_valid\": y_valid,\n        \"model\": pipe,\n    }\n\ndef show_directory(directory='/kaggle/input'):\n    for dirname, _, filenames in os.walk(directory):\n        print(f'Directory: {dirname}')\n\nprint('Helper functions, libraries, labels are imported successfully!')","metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Once you've done all your work on Kaggle, you can download the notebook from Kaggle. That way any work you did on Kaggle won't be lost. ","metadata":{}},{"cell_type":"markdown","source":"## Getting Started with Kaggle Datasets\n<hr>","metadata":{"execution":{"iopub.status.busy":"2025-03-08T21:08:21.506645Z","iopub.execute_input":"2025-03-08T21:08:21.507009Z","iopub.status.idle":"2025-03-08T21:08:21.512590Z","shell.execute_reply.started":"2025-03-08T21:08:21.506982Z","shell.execute_reply":"2025-03-08T21:08:21.511405Z"}}},{"cell_type":"markdown","source":"In this exercise, we'll use some Kaggle datasets for image classification. To get started with the dataset, follow the instructions below.\n\n1. On the right-hand side of your Kaggle notebook, find `Input` and click `+ Add Input`.\n\n2. Choose `Datasets`. In the search bar, type `cat-and-dog`. You will locate this [dataset]((https://www.kaggle.com/datasets/tongpython/cat-and-dog)) with a size of 228MB and click `+` if the dataset is not added yet.\n\n3. In the search bar, type `fruit-classification10-class`. You will locate this [dataset]((https://www.kaggle.com/datasets/karimabdulnabi/fruit-classification10-class)) with a size of 31MB and click `+` if the dataset is not added yet.","metadata":{}},{"cell_type":"markdown","source":"## Exercise 1: Using pre-trained models out of the box\n<hr>\n\nFirst, we will use pre-trained Convolutional Neural Network (CNN) models out of the box for image classification. You can find a list of available pre-trained models [here](https://nnabla.readthedocs.io/en/v1.39.0/python/api/models/imagenet.html).\n\nIn this exercise, we'll use the `VGG16` model to classify cats and dogs using this [dataset](https://www.kaggle.com/datasets/tongpython/cat-and-dog). \n\n**Run** the following cell and it will display the image, its label (`Actual Label`), the model's predictions (`Predicted Class`), and the corresponding probabilities (`Probability Score`).","metadata":{},"attachments":{}},{"cell_type":"code","source":"# YOUR INPUT: Number of images shown per category\nnum_images=4\n\n# Set up data\nIMAGE_DIR = {\n    \"Cat\": \"/kaggle/input/cat-and-dog/test_set/test_set/cats/*.*\", \n    \"Dog\": \"/kaggle/input/cat-and-dog/test_set/test_set/dogs/*.*\"\n}\n\n# Display image and prediction labels with probability\nfor true_label, image_path in IMAGE_DIR.items():\n    show_image_label_prob(image_path, true_label, num_images=num_images)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exercise 1.1 \n\n<div class=\"alert alert-info\">\n\n**Discussion questions**\n\n1. How well does the model distinguish between cats and dogs?\n2. Do you notice any specific patterns or characteristics in the Predicted Class versus the Actual Label?\n   \n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\">\n\nType your answer below.\n    \n</div>","metadata":{"execution":{"iopub.execute_input":"2024-12-06T22:24:07.519575Z","iopub.status.busy":"2024-12-06T22:24:07.518938Z","iopub.status.idle":"2024-12-06T22:24:07.524964Z","shell.execute_reply":"2024-12-06T22:24:07.523830Z","shell.execute_reply.started":"2024-12-06T22:24:07.519539Z"}}},{"cell_type":"markdown","source":"_Type your answer here, replacing this text._","metadata":{}},{"cell_type":"markdown","source":"Hopefully, you observed reasonable performance on the cats and dogs dataset. Now, let's test the model on a slightly different dataset: [Fruit Classification Dataset](https://www.kaggle.com/datasets/karimabdulnabi/fruit-classification10-class). \n\nThe dataset includes images from the following 10 classes:\n- Apple\n- Banana\n- Avocado\n- Cherry\n- Kiwi\n- Mango\n- Orange\n- Pineapple\n- Strawberries\n- Watermelon\n\n**Run** the following cell and it will display the image, its label (`Actual Label`), the model's predictions (`Predicted Class`), and the corresponding probabilities (`Probability Score`).","metadata":{}},{"cell_type":"code","source":"# YOUR INPUT: Number of images shown per category\nnum_images=2\n\n# Set up data\nIMAGE_DIR = {\n    \"Apple\": \"/kaggle/input/fruit-classification10-class/MY_data/train/Apple/*.jpeg\", \n    \"Banana\": \"/kaggle/input/fruit-classification10-class/MY_data/train/Banana/*.jpeg\", \n    \"Avocado\": \"/kaggle/input/fruit-classification10-class/MY_data/train/avocado/*.jpeg\", \n    \"Cherry\": \"/kaggle/input/fruit-classification10-class/MY_data/train/cherry/*.jpeg\", \n    \"Kiwi\": \"/kaggle/input/fruit-classification10-class/MY_data/train/kiwi/*.jpeg\", \n    \"Mango\": \"/kaggle/input/fruit-classification10-class/MY_data/train/mango/*.jpeg\", \n    \"Orange\": \"/kaggle/input/fruit-classification10-class/MY_data/train/orange/*.jpeg\", \n    \"Pineapple\": \"/kaggle/input/fruit-classification10-class/MY_data/train/pinenapple/*.jpeg\", \n    \"Strawberries\": \"/kaggle/input/fruit-classification10-class/MY_data/train/strawberries/*.jpeg\", \n    \"Watermelon\": \"/kaggle/input/fruit-classification10-class/MY_data/train/watermelon/*.jpeg\", \n}\n\n# Display image and prediction labels with probability\nfor true_label, image_path in IMAGE_DIR.items():\n    show_image_label_prob(image_path, true_label, num_images=num_images)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exercise 1.2\n\n<div class=\"alert alert-info\">\n\n**Discussion questions**\n\n1. How well does the model distinguish between different types of fruits?\n2. Did you notice any differences in the model's performance between the cats and dogs dataset and the fruits dataset? Briefly explain your answer. \n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\">\n\nType your answer below.\n    \n</div>","metadata":{"execution":{"iopub.execute_input":"2024-12-06T22:24:07.519575Z","iopub.status.busy":"2024-12-06T22:24:07.518938Z","iopub.status.idle":"2024-12-06T22:24:07.524964Z","shell.execute_reply":"2024-12-06T22:24:07.523830Z","shell.execute_reply.started":"2024-12-06T22:24:07.519539Z"}}},{"cell_type":"markdown","source":"_Type your answer here, replacing this text._","metadata":{}},{"cell_type":"markdown","source":"<br><br><br><br>","metadata":{}},{"cell_type":"markdown","source":"## Exercise 2: Using pre-trained models as feature extractors\n\n<br>\n\nOften, we want to train a model on our own datasets and have it predict classes specific to our data, rather than the 1000 classes from ImageNet. To achieve this, we can use pre-trained models as feature extractors. Specifically, we can leverage the rich representations learned by pre-trained models, use these representations as feature vectors, and train a new model on these feature vectors for our specific task.\n\nIn this exercise, you will use a pre-trained CNN model, `Densenet`, to extract features from images and train a logistic regression classifier to identify different types of fruits.\n\nTo get started, **run** the following cell to prepare the data.","metadata":{"execution":{"iopub.execute_input":"2024-11-24T01:31:04.573651Z","iopub.status.busy":"2024-11-24T01:31:04.573302Z","iopub.status.idle":"2024-11-24T01:31:04.578075Z","shell.execute_reply":"2024-11-24T01:31:04.577044Z","shell.execute_reply.started":"2024-11-24T01:31:04.573621Z"}}},{"cell_type":"code","source":"# Set up data\nDATA_DIR = '/kaggle/input/fruit-classification10-class/MY_data/'\nSUBDIR = {'train': 'train', 'valid': 'test'}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we'll extract feature vectors from the images above using the pre-trained model. \n\nEach image sample is represented with feature vectors (tabular data shown below) extracted from the `Densenet` model.\n\nWe'll then train a **logistic regression model** to classify fruits using the extracted features and the true label of the fruit images.\n\n**Run** the following cell to extract feature and train the model.","metadata":{}},{"cell_type":"code","source":"# YOUR INPUT: Integer `seed` to randomize the image dataset\nseed=315\n\n# Extract features and train ML model\nmodel_output = extract_features_and_train_classifier(DATA_DIR, SUBDIR, seed=seed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's **run** the cell below and have some predictions made by the model to examine together!","metadata":{}},{"cell_type":"code","source":"# YOUR INPUT: Number of images shown\nnum_images=25\n\n# Show predictions\nshow_predictions(model_output[\"model\"], model_output[\"Z_valid\"], model_output[\"y_valid\"], model_output[\"dataloaders\"][\"valid\"], model_output[\"class_names\"], num_images=num_images, seed=seed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exercise 2.1\n\n<div class=\"alert alert-info\">\n\n**Discussion questions**\n\n1. How well does the model distinguish between different types of fruits?\n2. Is the performance with feature extraction better than the performance with the out-of-the-box method? \n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\">\n\nType your answer below.\n    \n</div>","metadata":{"execution":{"iopub.execute_input":"2024-12-06T22:24:07.519575Z","iopub.status.busy":"2024-12-06T22:24:07.518938Z","iopub.status.idle":"2024-12-06T22:24:07.524964Z","shell.execute_reply":"2024-12-06T22:24:07.523830Z","shell.execute_reply.started":"2024-12-06T22:24:07.519539Z"}}},{"cell_type":"markdown","source":"_Type your answer here, replacing this text._","metadata":{}},{"cell_type":"markdown","source":"### Your Free Time (Optional)","metadata":{}},{"cell_type":"markdown","source":"**Your tasks**:\n\nChoose any image dataset that interests you and train a model using it!\n\nFeel free to discuss your ideas and progress with your teammates and the workshop team.","metadata":{}},{"cell_type":"code","source":"# YOUR INPUT\nDATA_DIR = \"{_DATA_DIRECTORY_PATH_}\"\nSUBDIR = \"{_SUB_DIRECTORY_PATH_}\"\nseed = \"{Seed_Integer}\"\n# Example - dataset `cat-breed-mardhik` https://www.kaggle.com/datasets/solothok/cat-breed\n# DATA_DIR = '/kaggle/input/cat-breed/cat-breed/'\n# SUBDIR = {'train': 'TRAIN', 'valid': 'TEST'}\n# seed = 315\n\n# Extract features and train ML model\nmodel_output = extract_features_and_train_classifier(DATA_DIR, SUBDIR, seed=seed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# YOUR INPUT: Number of images shown\nnum_images=20\n\n# Show predictions\nshow_predictions(model_output[\"model\"], model_output[\"Z_valid\"], model_output[\"y_valid\"], model_output[\"dataloaders\"][\"valid\"], model_output[\"class_names\"], num_images=num_images, seed=seed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<!-- END QUESTION -->\n\n<br><br>","metadata":{"deletable":true,"editable":true}}]}